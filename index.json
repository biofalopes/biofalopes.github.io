

















































[{"categories":["Terraform"],"contents":"In order to pass the Terraform Associate Exam I followed the Official guide as a baseline for my studies, as I usually do for certification exams, and have been working well.\nThe HashiCorp Terraform Associate is considered a foundational level certification, so you can expect to have essential knowledge and skills on the concepts and not necessarily lots of hands-on practice. If you already work with Terraform you probably be able to go very quicky over the topics, but it\u0026rsquo;s important to know some details and also some commands that are not so common. You must also know the features that exist on Terraform Enterprise packages and Terraform Cloud.\nImportant notes about the test: it is 57 questions long and is a proctored exam. You get 60 minutes to complete and you can mark questions for later review. The certification is valid for two years. Since Terraform is quickly and constantly evolving, it makes sense that it has a shorter validity than most certifications (three years).\nMy personal tips for this and any other certification exam: do lots of practice tests, go over every subject at least once even if you know it already from previous experience, make notes and don\u0026rsquo;t just study for the test, apply the knowledge in labs or personal projects. I would say that practicing is at least 2x more efficient in making the knowledge permanent than reading or watching videos.\nOfficial Exam Information from Hashicorp can be found here: HashiCorp Certified: Terraform Associate (002)\nObjective Description 1 Understand Infrastructure as Code (IaC) concepts 2 Understand Terraform\u0026rsquo;s purpose (vs other IaC) 3 Understand Terraform basics 4 Use the Terraform CLI (outside of core workflow) 5 Interact with Terraform modules 6 Navigate Terraform workflow 7 Implement and maintain state 8 Read, generate, and modify configuration 9 Understand Terraform Cloud and Enterprise capabilities What I\u0026rsquo;m gonna do here is go over each of the objectives and cover (almost) everything required to know in order to pass the exam. Most of the content is provided by the official study guide and the exam review, so I\u0026rsquo;ll be using them as reference. It\u0026rsquo;s probably gonna be a long read, but I rather have it all in one place than making several posts on the same subject.\n1. Understand Infrastructure as Code (IaC) concepts 1a. Explain what IaC is Infrastructure as code (IaC) tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share.\n1b. Describe advantages of IaC patterns IaC makes it easy to provision and apply infrastructure configurations by standardizing the workflow. This is accomplished by using a common syntax across a number of different infrastructure providers (e.g. AWS, GCP). Some key advantages of using IaC patterns are:\nReusability: you can write or use modules to reuse code that are common between different projects; Consistency: managing infrastructure manually are prone to errors; Idempotency: no matter how many times you run your IaC and, what your starting state is, you will end up with the same end state. This simplifies the provisioning of Infrastructure and reduces the chances of inconsistent results; Versioning and Source Control: by using a VCS like Git you can get visibility and security on your infrastructure. 2. Understand Terraform\u0026rsquo;s purpose (vs other IaC) 2a. Explain multi-cloud and provider-agnostic benefits Terraform is cloud-agnostic and allows a single configuration to be used to manage multiple providers, and to even handle cross-cloud dependencies. With Terraform, users can manage a heterogeneous environment with the same workflow by creating a configuration file to fit the needs of that platform or project. Terraform plugins called providers let Terraform interact with cloud platforms and other services via their application programming interfaces (APIs). HashiCorp and the Terraform community have written over 1,000 providers to manage resources on Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, and DataDog, just to name a few.\n2b. Explain the benefits of state Terraform keeps track of your real infrastructure in a state file, which acts as a source of truth for your environment. Terraform uses the state file to determine the changes to make to your infrastructure so that it will match your configuration.\n3. Understand Terraform basics 3a. Handle Terraform and provider installation and versioning Terraform can be installed using the user’s terminal: https://learn.hashicorp.com/tutorials/terraform/install-cli\nExamples:\n## Mac OS brew install terraform ## Windows choco install terraform ## Linux sudo apt install terraform There might be previous steps necessary according to your system, like installing the package manager or adding the repository.\nAlternatively, Terraform can be manually installed by downloading the binary to your computer.\n3b. Describe plug-in based architecture Terraform uses a plugin-based architecture to support hundreds of infrastructure and service providers. Initializing a configuration directory downloads and installs providers used in the configuration. Terraform plugins are compiled for a specific operating system and architecture, and any plugins in the root of the user’s plugins directory must be compiled for the current system. A provider is a plugin that Terraform uses to translate the API interactions with that platform or service.\nTerraform must initialize a provider before it can be used. The initialization process downloads and installs the provider\u0026rsquo;s plugin so that it can later be executed. Terraform knows which provider(s) to download based on what is declared in the configuration files. For example:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } The provider block can contain the following meta-arguments:\nversion - constrains which provider versions are allowed; alias - enables using the same provider with different configurations (e.g. provisioning resources in multiple AWS regions). HashiCorp recommends using provider requirements instead.\nBy default, a plugin is downloaded into a subdirectory of the working directory so that each working directory is self-contained. As a consequence, if there are multiple configurations that use the same provider then a separate copy of its plugin will be downloaded for each configuration. To manually install a provider, move it to:\n## MacOS / Linux ~/.terraform.d/plugins ## Windows %APPDATA%\\terraform.d\\plugins Given that provider plugins can be quite large, users can optionally use a local directory as a shared plugin cache. This is enabled through using the plugin_cache_dir setting in the CLI configuration file.\nplugin_cache_dir = \u0026#34;$HOME/.terraform.d/plugin-cache\u0026#34; This configuration ensures each plugin binary is downloaded only once.\n3c. Demonstrate using multiple providers To instantiate the same provider for multiple configurations, use the alias argument. For example, the AWS provider requires specifying the region argument. The following code block demonstrates how alias can be used to provision resources across multiple regions using the same configuration files.\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-1\u0026#34; alias = \u0026#34;pncda\u0026#34; } resource \u0026#34;aws_vpc\u0026#34; \u0026#34;vpc-pncda\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; provider = \u0026#34;aws.pncda\u0026#34; } 3d. Describe how Terraform finds and fetches providers Providers are released on a separate rhythm from Terraform itself, and so each provider has its own version number. For production use, consider constraining the acceptable provider versions in the configuration to ensure that new versions with breaking changes will not be automatically installed by terraform init in future.\nAny non-certified or third-party providers must be manually installed, since terraform init cannot automatically download them.\nThe required_version setting can be used to constrain which versions of Terraform can be used with the configuration.\nterraform { required_version = \u0026#34;\u0026gt;= 0.14.3\u0026#34; } The value for required_version is a string containing a comma-delimited list of constraints. Each constraint is an operator followed by a version number. The following operators are allowed:\nOperator\rUsage\rExample\r= (or no operator)\rUse exact version\r\"= 0.14.3\"\rMust use v0.14.3 !=\rVersion not equal\r\"!=0.14.3\"\rMust not use v0.14.3 \u003e or \u003e= or \u0026lt; or \u0026lt;=\rVersion comparison\r\"\u003e= 0.14.3\"\rMust use a version greater than or equal to v0.14.3\r~\u003e\rPessimistic constraint operator that both both the oldest and newest version allowed\r\"~\u003e= 0.14\"\rMust use a version greater than or equal to v0.14 but less than v0.15 (which includes v0.14.3)\rSimilarly, a provider version requirement can be specified. The following is an example limited the version of AWS provider:\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; version = \u0026#34;\u0026gt;=3.1.0\u0026#34; } It is recommended to use these operators in production to ensure the correct version is being used and avoid accidental upgrades that might have breaking changes.\n3e. Explain when to use and not use provisioners and when to use local-exec or remote-exec Provisioners can be used to model specific actions on the local machine or on a remote machine. For example, a provisioner can enable uploading files, running shell scripts, or installing or triggering other software (e.g. configuration management) to conduct initial setup on an instance. Provisioners are defined within a resource block:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-b374d5a5\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo hello \u0026gt; hello.txt\u0026#34; } } Multiple provisioner blocks can be used to define multiple provisioning steps.\nHashiCorp recommends that provisioners should only be used as a last resort.\nTypes of Provisioners This section will cover the various types of generic provisioners. There are also vendor specific provisioners for configuration management tools (e.g. Salt, Puppet).\n1. File The file provisioner is used to copy files or directories from the machine executing Terraform to the newly created resource.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { # ... provisioner \u0026#34;file\u0026#34; { source = \u0026#34;conf/myapp.conf\u0026#34; destination = \u0026#34;/etc/myapp.conf\u0026#34; } } The file provisioner supports both ssh and winrm type connections.\n2. local-exec The local-exec provisioner runs by invoking a process local to the user’s machine running Terraform. This is used to do something on the machine running Terraform, not the resource provisioned. For example, a user may want to create an SSH key on the local machine.\nresource \u0026#34;null_resource\u0026#34; \u0026#34;generate-sshkey\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;yes y | ssh-keygen -b 4096 -t rsa -C \u0026#39;terraform-kubernetes\u0026#39; -N \u0026#39;\u0026#39; -f ${var.kubernetes_controller.[\u0026#34;private_key\u0026#34;]}\u0026#34; } } 3. remote-exec Comparatively, remote-exec which invokes a script or process on a remote resource after it is created. For example, this may be used to bootstrap a newly provisioned cluster or to run a script.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { key_name = aws_key_pair.example.key_name ami = \u0026#34;ami-04590e7389a6e577c\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; connection { type = \u0026#34;ssh\u0026#34; user = \u0026#34;ec2-user\u0026#34; private_key = file(\u0026#34;~/.ssh/terraform\u0026#34;) host = self.public_ip } provisioner \u0026#34;remote-exec\u0026#34; { inline = [ \u0026#34;sudo amazon-linux-extras enable nginx1.12\u0026#34;, \u0026#34;sudo yum -y install nginx\u0026#34;, \u0026#34;sudo systemctl start nginx\u0026#34; ] } } Both ssh and winrm connections are supported.\nBy default, provisioners are executed when the defined resource is created and during updates or other parts of the lifecycle. It is intended to be used for bootstrapping a system. If a provisioner fails at creation time, the resource is marked as tainted. Terraform will plan to destroy and recreate the tainted resource at the next terraform apply command.\nBy default, when a provisioner fails, it will also cause the terraform apply command to fail. The on_failure parameter can be used to specify different behavior.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { # ... provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo The server\u0026#39;s IP address is ${self.private_ip}\u0026#34; on_failure = \u0026#34;continue\u0026#34; } } Expressions in provisioner blocks cannot refer to the parent resource by name. Use the self object to represent the provisioner\u0026rsquo;s parent resource (see previous example).\nAdditionally, provisioners can also be configured to run when the defined resource is destroyed. This is configured by specifying when = “destroy” within the provisioner block.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { # ... provisioner \u0026#34;local-exec\u0026#34; { when = \u0026#34;destroy\u0026#34; command = \u0026#34;echo \u0026#39;Destroy-time provisioner\u0026#39;\u0026#34; } } By default, a provisioner only runs at creation. To run a provisioned at deletion, it must be explicitly defined.\n4. Use the Terraform CLI (outside of core workflow) 4a. Given a scenario: choose when to use terraform fmt to format code The terraform fmt command is used to reformat Terraform configuration files in a canonical format and style. Using this command applies a subset of the Terraform language style conventions, along with other minor adjustments for readability. The following are common flags that may be used:\nterraform fmt -diff - to output differences in formatting; terraform fmt -recursive - apply format to subdirectories; terraform fmt -list=false - when formatting many files across a number of directories, use this to not list files formatted using command. The canonical format may change between Terraform versions, so it\u0026rsquo;s a good practice to run terraform fmt after upgrading. This should be done on any modules along with other changes being made to adopt the new version.\n4b. Given a scenario: choose when to use terraform taint to taint Terraform resources The terraform taint command manually marks a Terraform-managed resource as tainted, which marks the resource to be destroyed and recreated at the next apply command. Command usage resembles:\nterraform taint [options] address terraform taint aws_security_group.allow_all To taint a resource inside a module:\nterraform taint \u0026#34;module.kubernetes.aws_instance.k8_node[4]\u0026#34; The address argument is the address of the resource to mark as tainted. The address is in the resource address syntax. When tainting a resource, Terraform reads from the default state file (terraform.tfstate). To specify a different path, use:\nterraform taint -state=path This command does not modify infrastructure, but does modify the state file in order to mark a resource as tainted. Once a resource is marked as tainted, the next plan will show that the resource is to be destroyed and recreated. The next apply will implement this change.\nForcing the recreation of a resource can be useful to create a certain side-effect not configurable in the attributes of a resource. For example, re-running provisioners may cause a node to have a different configuration or rebooting the machine from a base image causes new startup scripts to execute.\nThe terraform untaint command is used to unmark a Terraform-managed resource as tainted, restoring it as the primary instance in the state.\nterraform untaint aws_security_group.allow_all Similarly, this command does not modify infrastructure, but does modify the state file in order to unmark a resource as tainted.\n4c. Given a scenario: choose when to use terraform import to import existing infrastructure into your Terraform state The terraform import command is used to import existing resources to be managed by Terraform. Effectively this imports existing infrastructure (created by other means) and allows Terraform to manage the resource, including destroy. The terraform import command finds the existing resource from ID and imports it into the Terraform state at the given address. Command usage resembles:\nterraform import [options] address ID terraform import aws_vpc.vpcda vpc-0a1be4pncda9 The ID is dependent on the resource type being imported. For example, for AWS instances the ID resembles i-abcd1234 whereas the zone ID for AWS Route53 resembles L69ZFG4TCUOZ1M.\nBecause any resource address is valid, the import command can import resources into modules as well directly into the root of state. Note that resources can be imported directly into modules.\n4d. Given a scenario: choose when to use terraform workspace to create workspaces Workspaces are technically equivalent to renaming a state file. Each Terraform configuration has an associated backend that defines how operations are executed and where persistent data (e.g. Terraform state) is stored. This persistent data stored in the backend belongs to a workspace. A default configuration has only one workspace named default with a single Terraform state. Some backends support multiple named workspaces, allowing multiple states to be associated with a single configuration. Command usage resembles:\nterraform workspace list terraform workspace new \u0026lt;name\u0026gt; terraform workspace show terraform workspace select \u0026lt;name\u0026gt; terraform workspace delete \u0026lt;name\u0026gt; The default workspace cannot be deleted.\nWorkspaces can be specified within configuration code. This example uses the workspace name as a tag:\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { tags = { Name = \u0026#34;web - ${terraform.workspace}\u0026#34; } # ... other arguments } A common use case for multiple workspaces is to create a parallel, distinct copy of a set of infrastructure in order to test a set of changes before modifying the main production infrastructure. For example, a developer working on a complex set of infrastructure changes might create a new temporary workspace in order to freely experiment with changes without affecting the default workspace.\nFor a local state configuration, Terraform stores the workspace states in a directory called terraform.tfstate.d.\n4e. Given a scenario: choose when to use terraform state to view Terraform state There are cases in which the Terraform state needs to be modified. Rather than modify the state directly, the terraform state commands should be used instead.\nThe terraform state list command is used to list resources within the state.\nThe command will list all resources in the state file matching the given addresses (if any). If no addresses are given, then all resources are listed. To specify a resource:\nterraform state list aws_instance.bar The terraform state pull command is used to manually download and output the state from remote state. This command downloads the state from its current location and outputs the raw format to stdout. While this command also works with local state, it is not very useful because users can see the local file.\nThe terraform state mv command is used to move items in the state. It can be used for simple resource renaming, moving items to and from a module, moving entire modules, and more. Because this command can also move data to a completely new state, it can be used to refactor one configuration into multiple separately managed Terraform configurations.\nterraform state mv [options] SOURCE DESTINATION terraform state mv \u0026#39;aws_instance.worker\u0026#39; \u0026#39;aws_instance.helper\u0026#39; The terraform state rm command is used to remove items from the state.\nterraform state rm \u0026#39;aws_instance.worker\u0026#39; terraform state rm \u0026#39;module.compute\u0026#39; It is important to note that items removed from the Terraform state are not physically destroyed, these items are simply no longer managed by Terraform. For example, if an AWS instance is deleted from the state, the AWS instance will continue running, but terraform plan will no longer manage that instance.\n4f. Given a scenario: choose when to enable verbose logging and what the outcome/value is Terraform has detailed logs that can be enabled by setting the TF_LOG environment variable to any value. This will cause detailed logs to appear on stderr.\nUsers can set TF_LOG to one of the log levels TRACE, DEBUG, INFO, WARN or ERROR to change the verbosity of the logs. TRACE is the most verbose and it is the default if TF_LOG is set to something other than a log level name.\nTo persist logged output users can set TF_LOG_PATH in order to force the log to always be appended to a specific file when logging is enabled. Note that even if TF_LOG_PATH is set, TF_LOG must be set in order for any logging to be enabled.\n5. Interact with Terraform modules 5a. Contrast module source options Modules can either be loaded from the local filesystem, or a remote source. Terraform supports a variety of remote sources, including the Terraform Registry, most version control systems, HTTP URLs, and Terraform Cloud or Terraform Enterprise private module registries.\n5b. Interact with module inputs and outputs The configuration that calls a module is responsible for setting the input values, which are passed as arguments in the module block. Input variables serve as parameters for a Terraform module, allowing aspects of the module to be customized without modifying the module’s code. It is common that most of the arguments to a module block will set variable values. Input variables allow modules to be shared between different configurations.\nWhen navigating the Terraform Registry, there is an \u0026ldquo;Inputs\u0026rdquo; tab for each module that describes all of the input variables supported.\nmodule \u0026#34;vpc\u0026#34; { source = \u0026#34;terraform-aws-modules/vpc/aws\u0026#34; name = \u0026#34;vpc\u0026#34; cidr = var.vpc_cidr azs = var.azs private_subnets = var.private_subnets public_subnets = var.public_subnets enable_nat_gateway = true enable_vpn_gateway = true tags = { Terraform = \u0026#34;true\u0026#34; Environment = var.name_env } } Module outputs are typically passed to other parts of the Terraform configuration or defined as outputs in the root module. Wherein, the output values are specified as:\nmodule.\u0026lt;MODULE NAME\u0026gt;.\u0026lt;OUTPUT NAME\u0026gt;\n5c. Describe variable scope within modules/child modules A resource defined in a module is encapsulated, so referencing the module cannot directly access its attributes. A child module can declare an output value to export select values to be accessible by the parent module. The contents of the outputs.tf file may contain code block(s) similar to:\noutput \u0026#34;vpc_public_subnets\u0026#34; { description = \u0026#34;IDs of the VPC\u0026#39;s public subnets\u0026#34; value = module.vpc.public_subnets } 5d. Discover modules from the public Terraform Module Registry The Terraform Registry is an interactive resource for discovering a wide selection of integrations (providers), configuration packages (modules), and security rules (policies) for use with Terraform. The Registry includes solutions developed by HashiCorp, third-party vendors, and the community.\nThe Terraform Registry is integrated directly into Terraform so you can directly specify providers and modules. Anyone can publish and consume providers, modules, and policies on the public Terraform Registry.\n5e. Defining module version Each module in the registry is versioned. These versions syntactically must follow semantic versioning.\nUse the version argument in the module block to specify versions:\nmodule \u0026#34;consul\u0026#34; { source = \u0026#34;hashicorp/consul/aws\u0026#34; version = \u0026#34;0.0.5\u0026#34; servers = 3 } More information about modules can be found here.\n6. Navigate Terraform workflow 6a. Describe Terraform workflow ( Write -\u0026gt; Plan -\u0026gt; Create ) The core Terraform workflow has three steps:\nWrite - Author infrastructure as code. Plan - Preview changes before applying. Apply - Provision reproducible infrastructure. This core workflow is a loop; the next time you want to make changes, you start the process over from the beginning. More information about the Terraform Workflow can be found here.\n6b. Initialize a Terraform working directory (terraform init) The terraform init command initializes a working directory containing Terraform configuration files. The command only initializes and never deletes an existing configuration or state, therefore is safe to run the command multiple times. During init, the root configuration directory is consulted for backend configuration and the selected backend is initialized using the defined configuration settings. The following are common flags used with the terraform init command:\nterraform init -upgrade - upgrade modules and plugins terraform init -backend=false - skip backend initialization terraform init -get=false - skip child module installation terraform init -get-plugins=false - skip plugin installation 6c. Validate a Terraform configuration (terraform validate) The terraform validate command validates the configuration files in a directory. Any remote services (i.e. remote state, provider APIs) are not validated, only the configuration files themselves. Validate runs checks to verify whether a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. This provides a general validation of the modules, including correctness of attribute names and value types.\nterraform validate requires an initialized working directory with any referenced plugins and modules installed.\n6d. Generate and review an execution plan for Terraform (terraform plan) The terraform plan command creates an execution plan. Terraform performs a refresh, unless explicitly disabled, and then determines what actions are required to achieve the desired state defined in the configuration files. The following are common flags used with the terraform plan command:\nterraform plan -out=tfplan - saves execution plan terraform apply tfplan - applies execution plan terraform plan -detailed-exitcode - returns a detailed exit code when the command exits terraform plan -refresh=true - update the state prior to plan 6e. Execute changes to infrastructure with Terraform (terraform apply) The terraform apply command applies the changes required for reaching the desired state of the configuration, or the predetermined set of actions generated by a terraform plan execution plan.\n6f. Destroy Terraform managed infrastructure (terraform destroy) The terraform destroy command is used to deprovision and destroy the Terraform-managed resources.\n7. Implement and maintain state 7a. Describe default local backend A Terraform backend determines how the state is loaded and how operations, such as terraform apply, are executed. This abstraction enables users to store sensitive state information in a different, secured location. Backends are configured with a nested backend block within the top-level terraform block. Only one backend block can be provided in a configuration. Backend configurations cannot have any interpolations or use any variables, thus must be hardcoded.\nBy default, Terraform uses the local backend. The local backend stores state on the local filesystem, locks that state using the system, and performs operations locally.\nterraform { backend \u0026#34;local\u0026#34; { path = \u0026#34;relative/path/to/terraform.tfstate\u0026#34; } } You don\u0026rsquo;t need to declare a local state block unless it\u0026rsquo;s desired for the backend to be a different location than the working directory. The backend defaults to terraform.tfstate relative to the root module.\n7b. Outline state locking When supported by the backend type (e.g. azurerm, S3, Consul), Terraform locks the state for all operations that could write state. This prevents others from acquiring the lock and potentially corrupting the state by attempting to write changes. Locking happens automatically on all applicable operations. If state locking fails, Terraform will not continue.\nState locking can be disabled for most commands using the -lock flag, however, this is not recommended. The terraform force-unlock command will manually unlock the state for the defined configuration. While this command does not modify the infrastructure resources, it does remove the lock on the state for the current configuration. Be very careful with this command, unlocking the state when another user holds the lock could cause multiple writers.\nterraform force-unlock LOCK_ID [DIR]\nThe force-unlock command should only be used in the scenario that automatic unlocking failed. As a means of protection, the force-unlock command requires a unique LOCK_ID. Terraform will output a LOCK_ID if automatic unlocking fails.\n7c. Handle backend authentication methods The Consul backend also requires a Consul access token. Per the recommendation of omitting credentials from the configuration and using other mechanisms, the Consul token would be provided by setting either the CONSUL_HTTP_TOKEN or CONSUL_HTTP_AUTH environment variables. See the documentation of your chosen backend to learn how to provide credentials to it outside of its main configuration.\n7d. Describe remote state storage mechanisms and supported standard backends With remote state, Terraform writes the state data to a remote data store, which can then be shared between all members of a team. Terraform supports storing state in Terraform Cloud, HashiCorp Consul, Amazon S3, Azure Blob Storage, Google Cloud Storage, Alibaba Cloud OSS, and more.\nRemote state is implemented by a backend or by Terraform Cloud, both of which you can configure in your configuration\u0026rsquo;s root module.\n7e. Describe effect of Terraform refresh on state The terraform refresh command is used to reconcile the state Terraform knows about (via its state file) with the real-world infrastructure. This can be used to detect any drift from the last-known state or to update the state file. While this command does not change the infrastructure, the state file is modified. This could cause changes to occur during the next plan or apply due to the modified state.\nThis does not modify infrastructure, but does modify the state file. If the state is changed, this may cause changes to occur during the next plan or apply.\nThis command is deprecated, because its default behavior is unsafe if you have misconfigured credentials for any of your providers.\nInstead of using refresh, you should use terraform apply -refresh-only. This alternative command will present an interactive prompt for you to confirm the detected changes. Wherever possible, avoid using terraform refresh explicitly and instead rely on Terraform\u0026rsquo;s behavior of automatically refreshing existing objects as part of creating a normal plan.\n7f. Describe backend block in configuration and best practices for partial configurations An example of a backend configuration would be:\nterraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;bachelol-bucket-tfstate\u0026#34; key = \u0026#34;dev/terraform.tfstate\u0026#34; region = \u0026#34;us-east-1\u0026#34; } } If a local state is then changed to an S3 backend, users will be prompted whether to copy existing state to the new backend when terraform init is executed. Similarly, if a remote backend configuration is removed, users will be prompted to migrate state back to local when terraform init is executed.\nWhen some or all of the arguments are omitted, it is called a partial configuration. Users do not have to specify every argument for the backend configuration. In some cases, omitting certain arguments may be desirable, such as to avoid storing secrets like access keys, within the main configuration. To provide the remaining arguments, users can do this using one of the following methods:\nFile: A configuration file may be specified via the init command line. To specify a file, use the -backend-config=PATH option when running terraform init. If the file contains secrets it may be kept in a secure data store, such as Vault, in which case it must be downloaded to the local disk before running Terraform. Command-line key/value pairs: Key/value pairs can be specified via the init command line. Note that many shells retain command-line flags in a history file, so this isn\u0026rsquo;t recommended for secrets. To specify a single key/value pair, use the -backend-config=\u0026quot;KEY=VALUE\u0026quot; option when running terraform init. Interactively: Terraform will interactively ask you for the required values, unless interactive input is disabled. Terraform will not prompt for optional values. When using partial configuration, it is required to specify an empty backend configuration in a Terraform configuration file. This specifies the backend type, such as:\nterraform { backend \u0026#34;consul\u0026#34; {} } An example of passing partial configuration with command-line key/value pairs:\n$ terraform init \\ -backend-config=\u0026#34;address=demo.consul.io\u0026#34; \\ -backend-config=\u0026#34;path=example_app/terraform_state\u0026#34; \\ -backend-config=\u0026#34;scheme=https\u0026#34; However, this is not recommended for secrets because many shells retain command-line flags in the history.\n7g. Understand secret management in state files When using local state, state is stored in plain-text JSON files.\nWhen using remote state, state is only ever held in memory when used by Terraform. It may be encrypted at rest, but this depends on the specific remote state backend.\nTerraform Cloud always encrypts state at rest and protects it with TLS in transit. Terraform Cloud also knows the identity of the user requesting state and maintains a history of state changes. This can be used to control access and track activity. Terraform Enterprise also supports detailed audit logging.\nThe S3 backend supports encryption at rest when the encrypt option is enabled. IAM policies and logging can be used to identify any invalid access. Requests for the state go over a TLS connection.\n8. Read, generate, and modify configuration 8a. Demonstrate use of variables and outputs Input variables: Input variables are a means to parameterize Terraform configurations. This is particularly useful for abstracting away sensitive values. These are defined using the variable block type:\nvariable \u0026#34;region\u0026#34; { default = \u0026#34;sa-east-1\u0026#34; } To then reference and use this variable, the resulting syntax would be var.region:\nprovider \u0026#34;aws\u0026#34; { region = var.region } There are three ways to assign configuration variables:\nCommand-line flags - use a command similar to terraform apply -var 'region=sa-east-1'; File - for persistent variable values, create a file called terraform.tfvars with the variable assignments; different files can be created for different environments (e.g. dev or prod); Environment variables - Terraform reads environment variables in the form of TF_VAR_name to find the variable value. For example, the TF_VAR_region variable can be set in the shell to set the Terraform variable for a region. Environment variables can only populate string-type variables; list and map type variables must be populated using another mechanism; Interactively - if terraform apply is executed with any variable unspecified, Terraform prompts users to input the values interactively; while these values are not saved, it does provide a convenient workflow when getting started. Strings and numbers are the most commonly used variables, but lists (arrays) and maps (hashtables or dictionaries) can also be used. Lists are defined either explicitly or implicitly. Example:\n## Declare implicitly by using brackets [] variable \u0026#34;cidrs\u0026#34; { default = [] } ## Declare explicitly with \u0026#39;list\u0026#39; variable \u0026#34;cidrs\u0026#34; { type = list } ## Specify list values in terraform.tfvars file cidrs = [ \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;10.1.0.0/16\u0026#34; ] A map is a key/value data structure that can contain other keys and values. Maps are a way to create variables that are lookup tables. Example:\n## Declare variables variable \u0026#34;region\u0026#34; {} variable \u0026#34;amis\u0026#34; { type = \u0026#34;map\u0026#34; default = { \u0026#34;us-east-1\u0026#34; = \u0026#34;ami-b374d5a5\u0026#34; \u0026#34;us-west-2\u0026#34; = \u0026#34;ami-fc0b939c\u0026#34; } } ## Reference variables resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = var.amis[var.region] instance_type = \u0026#34;t2.micro\u0026#34; } The above mechanisms for setting variables can be used together in any combination. If the same variable is assigned multiple values, Terraform uses the last value it finds, overriding any previous values. Terraform loads variables in the following order, with later sources taking precedence over earlier ones:\nenvironment variables; the terraform.tfvars file, if present; the terraform.tfvars.json file, if present; any *.auto.tfvars or *.auto.tfvars.json files, processed in lexical order of their filenames; any -var and -var-file options on the command line, in the order provided (this includes variables set by a Terraform Cloud workspace). The same variable cannot be assigned multiple values within a single source.\nOutput variables: An output variable is a way to organize data to be easily queried and shown back to the Terraform user. Outputs are a way to tell Terraform what data is important and should be referenceable. This data is output when apply is called and can be queried using the terraform output command.\n## Define Resources resource \u0026#34;aws_instance\u0026#34; \u0026#34;importugues\u0026#34; { ami = \u0026#34;ami-08d70e59c07c61a3a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } resource \u0026#34;aws_eip\u0026#34; \u0026#34;ip\u0026#34; { vpc = true instance = aws_instance.importugues.id } ## Define Output output \u0026#34;ip\u0026#34; { value = aws_eip.ip.public_ip } The example specifies to output the public_ip attribute. Run terraform apply to populate the output. The output can also be viewed by using the terraform output ip command, with ip in this example referencing the defined output.\n8b. Describe secure secret injection best practice Some providers, such as AWS, allow users to store credentials in a separate configuration file. It is a good practice in these instances to store credential keys in a config file such as .aws/credentials and not in the Terraform code.\nVault is a secrets management system that allows users to secure, store and tightly control access to tokens, passwords, certificates, encryption keys for protecting secrets and other sensitive data using a UI, CLI, or HTTP API.\nIt is recommended that the terraform.tfstate or .auto.tfvars files should be ignored by Git when committing code to a repository. The terraform.tfvars file may contain sensitive data, such as passwords or IP addresses of an environment that should not be shared with others.\n8c. Understand the use of collection and structural types A complex type groups multiple values into a single value. There are two categories of complex types:\nCollection types for grouping similar values: list(...) - a sequence of values identified by consecutive whole numbers starting with zero; map(...) - a collection of values where each is identified by a string label; set(...) - a collection of unique values that do not have any secondary identifiers or ordering. Structural types for grouping potentially dissimilar values: object(...) - a collection of named attributes that each have their own type; tuple(...) - a sequence of elements identified by consecutive whole numbers starting with zero, where each element has its own type. 8d. Create and differentiate resource and data configuration Resources Resources are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as a compute instance.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = \u0026#34;ami-a1b2c3d4\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } A resource block declares a resource of a given type (\u0026quot;aws_instance\u0026quot;) with a given local name (\u0026quot;web\u0026quot;). The name is used to refer to this resource from elsewhere in the same Terraform module.\nData Sources Data Sources are the way for Terraform to query a platform (e.g. AWS) and retrieve data (e.g. API request to get information). The use of data sources enables Terraform configuration to make use of information defined outside of this Terraform configuration. A provider determines what data sources are available alongside its set of resource types.\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;example\u0026#34; { most_recent = true owners = [\u0026#34;self\u0026#34;] tags = { Name = \u0026#34;app-server\u0026#34; Tested = \u0026#34;true\u0026#34; } } A data block lets Terraform read data from a given data source type (aws_ami) and export the result under the given local name (example). Data source attributes are interpolated with the syntax data.TYPE.NAME.ATTRIBUTE. To reference the data source example above, use data.aws_ami.example.id.\n8e. Use resource addressing and resource parameters to connect resources together A Resource Address is a string that references a specific resource in a larger infrastructure. An address is made up of two parts:\n[module path][resource spec] A resource spec addresses a specific resource in the config. It takes the form of:\nresource_type.resource_name[resource index] In which these constructs are defined as:\nresource_type - type of the resource being addressed resource_name - user-defined name of the resource resource index] - an optional index into a resource with multiple instances, surrounded by square braces [ and ]. For full reference to values, see here.\n8f. Use Terraform built-in functions to write configuration The Terraform language includes a number of built-in functions that you can call from within expressions to transform and combine values. The general syntax for function calls is a function name followed by comma-separated arguments in parentheses:\nmax(5, 12, 9) The following is a non-exhaustive list of built-in functions:\nfilebase64(path) - reads the contents of a file at the given path and returns as a base64-encoded string formatdate(spec, timestamp) - converts a timestamp into a different time format jsonencode({\u0026quot;hello\u0026quot;=\u0026quot;world\u0026quot;}) - encodes a given value to a string using JSON syntax cidrhost(\u0026quot;10.12.127.0/20\u0026quot;, 16) - calculates a full host IP address for a given host number within a given IP network address prefix file - reads the contents of a file and returns as a string flatten - takes a list and replaces any elements that are list with a flattened sequence of the list contents lookup - retrieves the value of a single element from a map, given its key. If the given key does not exist, a the given default value is returned instead The Terraform language does not support user-defined functions, therefore only the built-in functions are available for use.\n8g. Configure resource using a dynamic block Some resource types include repeatable nested blocks in their arguments. Users can dynamically construct repeatable nested blocks like setting using a special dynamic block type, which is supported inside resource, data, provider, and provisioner blocks:\nresource \u0026#34;aws_elastic_beanstalk_environment\u0026#34; \u0026#34;bean\u0026#34; { name = \u0026#34;tf-beanstalk\u0026#34; application = \u0026#34;${aws_elastic_beanstalk_application.tftest.name}\u0026#34; solution_stack_name = \u0026#34;64bit Amazon Linux 2018.03 v2.11.4 running Go 1.12.6\u0026#34; dynamic \u0026#34;setting\u0026#34; { for_each = var.settings content { namespace = setting.value[\u0026#34;namespace\u0026#34;] name = setting.value[\u0026#34;name\u0026#34;] value = setting.value[\u0026#34;value\u0026#34;] } } } A dynamic block acts similar to a for expression, but instead produces nested blocks rather than a complex typed value. It iterates over a given complex value, and generates a nested block for each element of that complex value.\n8h. Describe built-in dependency management (order of execution based) 9. Understand Terraform Cloud and Enterprise capabilities 9a. Describe the benefits of Sentinel, registry, and workspaces Sentinel: Sentinel is a policy as code framework that enables the same practices to be applied to enforcing and managing policy as used for infrastructure. These policies fall into a few categories:\nCompliance - ensuring adherence to external standards like GDPR or PCI-DSS Security - ensuring protection of data privacy and infrastructure integrity (i.e. exposing only certain ports) Operational Excellence - preventing outages or service degradations (i.e. n+1 minimums) Sentinel has been integrated into Terraform Enterprise.\nRegistry: The Module Registry gives Terraform users easy access to templates for setting up and running infrastructure with verified and community modules.\nTerraform Cloud\u0026rsquo;s private module registry helps users share Terraform modules across an organization. It includes support for module versioning, a searchable and filterable list of available modules, and a configuration designer to help users build new workspaces faster.\nBy design, the private module registry works similarly to the public registry.\nWorkspaces: Using Terraform CLI, it is the working directory used to manage collections of resources. But, this is where Terraform Cloud differs: workspaces are used to collect and organize infrastructure instead of directories. A workspace contains everything Terraform needs to manage a given collection of infrastructure, and separate workspaces function like completely separate working directories.\nTerraform Cloud and Terraform CLI both have features called workspaces, but the features are slightly different. CLI workspaces are alternate state files in the same working directory; a convenience feature for using one configuration to manage multiple similar groups of resources.\n9b. Differentiate OSS and Terraform Cloud workspaces Feature\rOSS\rTerraform Cloud\rEnterprise\rTerraform Configuration\rLocal or version control repo\rVersion control repo or periodically updated via CLI/API\rSame as Terraform Cloud, but also has the following features:\rSAML/SSO\rAudit Logs\rPrivate Network Connectivity\rClustering\rVariable Values\rAs .tfvars file, as CLI arguments, or in shell environment\rIn workspace\rState\rOn disk or in remote backend\rIn workspace\rCredential and Secrets\rIn shell environments or prompted\rIn workspace, stored as sensitive variables\r9c. Summarize features of Terraform Cloud Terraform Cloud is an application that helps teams use Terraform together. It manages Terraform runs in a consistent and reliable environment, and includes easy access to shared state and secret data, access controls for approving changes to infrastructure, a private registry for sharing Terraform modules, detailed policy controls for governing the contents of Terraform configurations, and more.\nTerraform Cloud is available as a hosted service at https://app.terraform.io. Small teams can sign up for free to connect Terraform to version control, share variables, run Terraform in a stable remote environment, and securely store remote state. Paid tiers allow you to add more than five users, create teams with different levels of permissions, enforce policies before creating infrastructure, and collaborate more effectively.\nThe Business tier allows large organizations to scale to multiple concurrent runs, create infrastructure in private environments, manage user access with SSO, and automates self-service provisioning for infrastructure end users.\nReferences Official Study Guide\nOfficial Exam Review\nPrepare for Certification\nPluralsight Terraform Path\nNed Bellavance\u0026rsquo;s GitHub\nTerraform for the Absolute Beginners with Labs\nMarcelo Andrade - /dev/sres\n","date":"11 Aug, 2022","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-cert_hu38d2a56a2d90f40618aecde9322f21e3_57176_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-cert_hu38d2a56a2d90f40618aecde9322f21e3_57176_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-cert_hu38d2a56a2d90f40618aecde9322f21e3_57176_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-cert_hu38d2a56a2d90f40618aecde9322f21e3_57176_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/terraform/terraform-cert_hu38d2a56a2d90f40618aecde9322f21e3_57176_1110x0_resize_box_3.png\" alt=\"\" width=\"819\" height=\"267\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/terraform-study-guide/","tags":["Terraform"],"title":"HashiCorp Certified: Terraform Associate Exam"},{"categories":["Terraform","AWS"],"contents":"This time I’ll be using an environment with a Load Balancer and an Autoscaling Group, which is a more complex architecture than the previous one.\nWe\u0026rsquo;ll have the following resources deployed with this procedure:\n1 VPC 2 Subnets 1 Internet Gateway 1 Route Table (with a default route and an association) 2 Security groups 1 Key Pair 1 EC2 Instance 1 Launch Configuration 1 ELB 1 ASG 2 Autoscaling Policies 2 Cloudwatch Metric Alarms It\u0026rsquo;s a very simple and you can build on top of it according to your needs, like the previous infrastructure. The code will be organized in the following structure:\nmain.tf: resource declaration providers.tf: aws provider configuration outputs.tf: output declaration datasources.tf: we\u0026rsquo;ll get the AMI information here userdata.tpl: script to be executed on first boot I\u0026rsquo;ll put the code first and then explain the most important parts. Let\u0026rsquo;s start with providers.tf:\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { shared_credentials_file = \u0026#34;/home/mando/.aws/credentials\u0026#34; profile = \u0026#34;mando\u0026#34; region = \u0026#34;us-east-1\u0026#34; } Terraform requires credentials to access your account on AWS. You can choose different approaches for that, like putting the keys directly instad of pointing to a credentials file. Although that works and might even be easier, you\u0026rsquo;ll probably have your code pushed into a git repository which would then expose the keys. More information on how to configure the provider can be found here -\u0026gt; Docs overview | hashicorp/aws.\nNow for the main.tf:\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;mando_vpc\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; enable_dns_hostnames = true tags = { Name = \u0026#34;Mando VPC\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;mando_public_subnet_us_east_1a\u0026#34; { vpc_id = aws_vpc.mando_vpc.id cidr_block = \u0026#34;10.10.1.0/24\u0026#34; availability_zone = \u0026#34;us-east-1a\u0026#34; tags = { Name = \u0026#34;Mando Public Subnet US-East 1a\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;mando_public_subnet_us_east_1b\u0026#34; { vpc_id = aws_vpc.mando_vpc.id cidr_block = \u0026#34;10.10.2.0/24\u0026#34; availability_zone = \u0026#34;us-east-1b\u0026#34; tags = { Name = \u0026#34;Mando Public Subnet US-East 1b\u0026#34; } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;mando_vpc_igw\u0026#34; { vpc_id = aws_vpc.mando_vpc.id tags = { Name = \u0026#34;Mando VPC - Internet Gateway\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;mando_vpc_public\u0026#34; { vpc_id = aws_vpc.mando_vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.mando_vpc_igw.id } tags = { Name = \u0026#34;Public Subnets Route Table for Mando VPC\u0026#34; } } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;mando_vpc_us_east_1a_public\u0026#34; { subnet_id = aws_subnet.mando_public_subnet_us_east_1a.id route_table_id = aws_route_table.mando_vpc_public.id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;mando_vpc_us_east_1b_public\u0026#34; { subnet_id = aws_subnet.mando_public_subnet_us_east_1b.id route_table_id = aws_route_table.mando_vpc_public.id } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;allow_http\u0026#34; { name = \u0026#34;allow_http\u0026#34; description = \u0026#34;Allow HTTP inbound connections\u0026#34; vpc_id = aws_vpc.mando_vpc.id ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;Allow HTTP Security Group\u0026#34; } } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;mando_key\u0026#34; { key_name = \u0026#34;mando-key\u0026#34; public_key = \u0026#34;ssh-rsa AAAABACH3L0Lc2EAAAADAQAPNCDAgQDELie/jIMM8uno12enId2YTmTjK1OGZJtTJFoSPdXIwn79qpZYQ3WXL8PlI/8dqFyGXvQj5bGJbgEydjSYVHFXFhPr4sdKcjguWbu895EjK2DgalcYuC1+6jBbFxiodoObsc+84m81+BACH3L0LQU3cm/rNKufrh6d21jIe4sQVul+WzJ9E8aPk34rPmRPgjYvh1T/P2hdgiUyJmKqOtDYwpokDRad+3W+iwGfoBACH3L0LoCWJ2rYzz6j80FKoiHm9cnSXvErezT7aAdenVzY3nEE4ylnHWVUdmzXN7IbCSLsDV3sdn0+c5E6oDX2/k1VwtSQ8TrUblM7AdpuB4ADniUSYvLqjd/NBIiHODzV6qZxXqoltVTsrTpbCWf1A063PBACH3L0L/F3mxBihWRAKfD1iqqfMXmYvAPosOkJ3u1yuwy/eCi6Q3SmA5n0vBSVKmYdUB9yQdAimWcUqabRzXLz+g8BrUxCBHwOf4+IZAp2AseJeoDQs0aqMwybr/k= mando\u0026#34; # replace with your key } resource \u0026#34;aws_launch_configuration\u0026#34; \u0026#34;web\u0026#34; { name_prefix = \u0026#34;web-\u0026#34; image_id = data.aws_ami.server_ami.id instance_type = \u0026#34;t2.micro\u0026#34; key_name = aws_key_pair.mando_key.id security_groups = [aws_security_group.allow_http.id] associate_public_ip_address = true user_data = file(\u0026#34;userdata.tpl\u0026#34;) lifecycle { create_before_destroy = true } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;elb_http\u0026#34; { name = \u0026#34;elb_http\u0026#34; description = \u0026#34;Allow HTTP traffic to instances through Elastic Load Balancer\u0026#34; vpc_id = aws_vpc.mando_vpc.id ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;Allow HTTP through ELB Security Group\u0026#34; } } resource \u0026#34;aws_elb\u0026#34; \u0026#34;web_elb\u0026#34; { name = \u0026#34;web-elb\u0026#34; security_groups = [ aws_security_group.elb_http.id ] subnets = [ aws_subnet.mando_public_subnet_us_east_1a.id, aws_subnet.mando_public_subnet_us_east_1b.id ] cross_zone_load_balancing = true health_check { healthy_threshold = 2 unhealthy_threshold = 2 timeout = 3 interval = 30 target = \u0026#34;HTTP:80/\u0026#34; } listener { lb_port = 80 lb_protocol = \u0026#34;http\u0026#34; instance_port = \u0026#34;80\u0026#34; instance_protocol = \u0026#34;http\u0026#34; } } resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;web\u0026#34; { name = \u0026#34;${aws_launch_configuration.web.name}-asg\u0026#34; min_size = 1 desired_capacity = 2 max_size = 4 health_check_type = \u0026#34;ELB\u0026#34; load_balancers = [ aws_elb.web_elb.id ] launch_configuration = aws_launch_configuration.web.name enabled_metrics = [ \u0026#34;GroupMinSize\u0026#34;, \u0026#34;GroupMaxSize\u0026#34;, \u0026#34;GroupDesiredCapacity\u0026#34;, \u0026#34;GroupInServiceInstances\u0026#34;, \u0026#34;GroupTotalInstances\u0026#34; ] metrics_granularity = \u0026#34;1Minute\u0026#34; vpc_zone_identifier = [ aws_subnet.mando_public_subnet_us_east_1a.id, aws_subnet.mando_public_subnet_us_east_1b.id ] # Required to redeploy without an outage. lifecycle { create_before_destroy = true } tag { key = \u0026#34;Name\u0026#34; value = \u0026#34;web\u0026#34; propagate_at_launch = true } } resource \u0026#34;aws_autoscaling_policy\u0026#34; \u0026#34;web_policy_up\u0026#34; { name = \u0026#34;web_policy_up\u0026#34; scaling_adjustment = 1 adjustment_type = \u0026#34;ChangeInCapacity\u0026#34; cooldown = 300 autoscaling_group_name = aws_autoscaling_group.web.name } resource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;web_cpu_alarm_up\u0026#34; { alarm_name = \u0026#34;web_cpu_alarm_up\u0026#34; comparison_operator = \u0026#34;GreaterThanOrEqualToThreshold\u0026#34; evaluation_periods = \u0026#34;2\u0026#34; metric_name = \u0026#34;CPUUtilization\u0026#34; namespace = \u0026#34;AWS/EC2\u0026#34; period = \u0026#34;120\u0026#34; statistic = \u0026#34;Average\u0026#34; threshold = \u0026#34;60\u0026#34; dimensions = { AutoScalingGroupName = aws_autoscaling_group.web.name } alarm_description = \u0026#34;This metric monitor EC2 instance CPU utilization\u0026#34; alarm_actions = [aws_autoscaling_policy.web_policy_up.arn] } resource \u0026#34;aws_autoscaling_policy\u0026#34; \u0026#34;web_policy_down\u0026#34; { name = \u0026#34;web_policy_down\u0026#34; scaling_adjustment = -1 adjustment_type = \u0026#34;ChangeInCapacity\u0026#34; cooldown = 300 autoscaling_group_name = aws_autoscaling_group.web.name } resource \u0026#34;aws_cloudwatch_metric_alarm\u0026#34; \u0026#34;web_cpu_alarm_down\u0026#34; { alarm_name = \u0026#34;web_cpu_alarm_down\u0026#34; comparison_operator = \u0026#34;LessThanOrEqualToThreshold\u0026#34; evaluation_periods = \u0026#34;2\u0026#34; metric_name = \u0026#34;CPUUtilization\u0026#34; namespace = \u0026#34;AWS/EC2\u0026#34; period = \u0026#34;120\u0026#34; statistic = \u0026#34;Average\u0026#34; threshold = \u0026#34;10\u0026#34; dimensions = { AutoScalingGroupName = aws_autoscaling_group.web.name } alarm_description = \u0026#34;This metric monitor EC2 instance CPU utilization\u0026#34; alarm_actions = [aws_autoscaling_policy.web_policy_down.arn] } Since it is a big file, let\u0026rsquo;s divide it in parts:\nVPC, subnets, route tables and routes, internet gateway, security group and key pair: the foundation or basic part of the infrastructure; Launch Configuration: this is where we define the parameters for the EC2 instances that the Autoscaling Group will launch. It\u0026rsquo;s very similar to when we deploy a single EC2 instance; Elastic Load Balancer: we deploy a security group specifically for the ELB first, and then the ELB itself. We have to specify which subnets, the listener and the health checks we\u0026rsquo;ll use; Autoscaling Group: this is the most important part regarding on how the infrastructure will \u0026ldquo;behave\u0026rdquo;, let\u0026rsquo;s say. We define a minimum, desired and maximum number of instances; specify what is going to be the health check, in this case is ELB so instance availability will be provided by it; then we set some Cloudwatch metrics to provide observability and add tags to identify the instances. outputs.tf:\noutput \u0026#34;elb_dns_name\u0026#34; { value = aws_elb.web_elb.dns_name } The only output we have is the DNS name of the ELB, which we\u0026rsquo;ll need to access it.\nuserdata.tpl:\n#!/bin/bash sudo apt -y update \u0026amp;\u0026amp; sudo apt -y install \\ nginx \u0026amp;\u0026amp; echo \u0026#34;$(curl http://169.254.169.254/latest/meta-data/local-ipv4)\u0026#34; \u0026gt; /usr/share/nginx/html/index.html sudo systemctl enable nginx sudo systemctl start nginx This userdata will install nginx on the EC2 instance at the first boot.\n","date":"01 Aug, 2022","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/aws/banner-1_huf32f69368812028e6b8f65305ed61699_494857_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/aws/banner-1_huf32f69368812028e6b8f65305ed61699_494857_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/aws/banner-1_huf32f69368812028e6b8f65305ed61699_494857_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/aws/banner-1_huf32f69368812028e6b8f65305ed61699_494857_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/aws/banner-1_huf32f69368812028e6b8f65305ed61699_494857_1110x0_resize_box_3.png\" alt=\"\" width=\"1175\" height=\"375\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/terraform-aws-asg/","tags":["Terraform","AWS"],"title":"Managing AWS Autoscaling Groups and Load Balancers using Terraform"},{"categories":["Terraform","AWS"],"contents":"It\u0026rsquo;s good to have a basic terraform code that deploys a basic environment on AWS whenever you need to run some quick tests on a free tier account.\nWe\u0026rsquo;ll have the following resources deployed with this procedure:\n1 VPC 1 Subnet 1 Internet Gateway 1 Route Table (with a default route and an association) 1 Security group 1 Key Pair 1 EC2 Instance It\u0026rsquo;s a very simple and you can build on top of it according to your needs, like adding a RDS instance or more EC2 instances, changing the instance type and so on. The code will be organized in the following structure:\nmain.tf: resource declaration providers.tf: aws provider configuration outputs.tf: output declaration datasources.tf: we\u0026rsquo;ll get the AMI information here variables.tf: only to inform our OS terraform.tfvars: same as previous userdata.tpl: script to be executed on first boot linux-ssh-config.tpl: SSH config for Linux windows-ssh-config.tpl: SSH config for Windows I added a local-exec provisioner that adds the host information in the ssh configuration, and with that we can then use it with VS Code and Remote SSH extension to connect directly to it and run our code.\nI\u0026rsquo;ll put the code first and then explain the most important parts. Let\u0026rsquo;s start with providers.tf:\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { shared_credentials_file = \u0026#34;/home/mando/.aws/credentials\u0026#34; profile = \u0026#34;mando\u0026#34; region = \u0026#34;us-east-1\u0026#34; } Terraform requires credentials to access your account on AWS. You can choose different approaches for that, like putting the keys directly instad of pointing to a credentials file. Although that works and might even be easier, you\u0026rsquo;ll probably have your code pushed into a git repository which would then expose the keys. More information on how to configure the provider can be found here -\u0026gt; Docs overview | hashicorp/aws.\nNow for the main.tf:\nresource \u0026#34;aws_vpc\u0026#34; \u0026#34;mando_vpc\u0026#34; { cidr_block = \u0026#34;10.10.0.0/16\u0026#34; enable_dns_hostnames = true enable_dns_support = true tags = { Name = \u0026#34;dev\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;mando_public_subnet\u0026#34; { vpc_id = aws_vpc.mando_vpc.id cidr_block = \u0026#34;10.10.1.0/24\u0026#34; map_public_ip_on_launch = true availability_zone = \u0026#34;us-east-1b\u0026#34; tags = { Name = \u0026#34;dev-public\u0026#34; } } resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;mando_internet_gw\u0026#34; { vpc_id = aws_vpc.mando_vpc.id tags = { Name = \u0026#34;dev-igw\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;mando_public_rt\u0026#34; { vpc_id = aws_vpc.mando_vpc.id tags = { Name = \u0026#34;dev-public-rt\u0026#34; } } resource \u0026#34;aws_route\u0026#34; \u0026#34;mando_default_route\u0026#34; { route_table_id = aws_route_table.mando_public_rt.id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.mando_internet_gw.id } resource \u0026#34;aws_main_route_table_association\u0026#34; \u0026#34;mando_public_assoc\u0026#34; { vpc_id = aws_vpc.mando_vpc.id route_table_id = aws_route_table.mando_public_rt.id } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;mando_sg\u0026#34; { name = \u0026#34;mando-sg\u0026#34; description = \u0026#34;mando Security Group\u0026#34; vpc_id = aws_vpc.mando_vpc.id ingress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;200.199.198.197/32\u0026#34;] # Replace with your public IP } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;mando-sg\u0026#34; } } resource \u0026#34;aws_key_pair\u0026#34; \u0026#34;mando_key\u0026#34; { key_name = \u0026#34;mando-key\u0026#34; public_key = \u0026#34;ssh-rsa AAAABACH3L0Lc2EAAAADAQAPNCDAgQDELie/jIMM8uno12enId2YTmTjK1OGZJtTJFoSPdXIwn79qpZYQ3WXL8PlI/8dqFyGXvQj5bGJbgEydjSYVHFXFhPr4sdKcjguWbu895EjK2DgalcYuC1+6jBbFxiodoObsc+84m81+BACH3L0LQU3cm/rNKufrh6d21jIe4sQVul+WzJ9E8aPk34rPmRPgjYvh1T/P2hdgiUyJmKqOtDYwpokDRad+3W+iwGfoBACH3L0LoCWJ2rYzz6j80FKoiHm9cnSXvErezT7aAdenVzY3nEE4ylnHWVUdmzXN7IbCSLsDV3sdn0+c5E6oDX2/k1VwtSQ8TrUblM7AdpuB4ADniUSYvLqjd/NBIiHODzV6qZxXqoltVTsrTpbCWf1A063PBACH3L0L/F3mxBihWRAKfD1iqqfMXmYvAPosOkJ3u1yuwy/eCi6Q3SmA5n0vBSVKmYdUB9yQdAimWcUqabRzXLz+g8BrUxCBHwOf4+IZAp2AseJeoDQs0aqMwybr/k= mando\u0026#34; # replace with your key } resource \u0026#34;aws_instance\u0026#34; \u0026#34;mando_node\u0026#34; { ami = data.aws_ami.server_ami.id instance_type = \u0026#34;t2.micro\u0026#34; key_name = aws_key_pair.mando_key.id vpc_security_group_ids = [aws_security_group.mando_sg.id] subnet_id = aws_subnet.mando_public_subnet.id user_data = file(\u0026#34;userdata.tpl\u0026#34;) root_block_device { volume_size = 10 } provisioner \u0026#34;local-exec\u0026#34; { command = templatefile(\u0026#34;${var.host_os}-ssh-config.tpl\u0026#34;, { hostname = self.public_ip, user = \u0026#34;ubuntu\u0026#34;, identityfile = \u0026#34;~/.ssh/id_rsa\u0026#34; }) interpreter = var.host_os == \u0026#34;windows\u0026#34; ? [\u0026#34;powershell\u0026#34;, \u0026#34;-Command\u0026#34;] : [\u0026#34;bash\u0026#34;, \u0026#34;-c\u0026#34;] } tags = { Name = \u0026#34;mando-node\u0026#34; } } We\u0026rsquo;ll not be using the default VPC or any other that was previously deployed, so we start by creating a VPC, a subnet and an Internet Gateway. We then have to create the route table, the default route and associate it with our VPC.\nAfter that, we create a security group and configure it to allow TCP access from our public IP. This of course can be adjusted according to specific needs, so I used a simple example that is not restrictive regarding what can be accessed, but very restrictive in the source IP. This prevents any access from outside, but might stop you from working with more people or from different places, and you\u0026rsquo;ll have to adjust the SG whenever your public ip changes.\nThen we create a key pair, and here you can use an existing one or generate a specific for this case, and you can also point to a file instead of pasting the key on the code.\nAnd last, but not least important, we specify an EC2 instance. The instance type is one that falls into free tier, and the root_block_device block is used to change the default size to 10. This can also be changed accordingly.\ndatasources.tf:\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;server_ami\u0026#34; { most_recent = true owners = [\u0026#34;099720109477\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*\u0026#34;] } } We use a data block to determine the AMI ID, since it changes between regions. We also use a filter block with a * at the end of the name, so we always pick the latest version. This can be changed when you want to lock in a specific version, which would be a best practice in production environments. Since we\u0026rsquo;re just testing, we might use the updated image with the latest security fixes.\nvariables.tf:\nvariable \u0026#34;host_os\u0026#34; { type = string } terraform.tfvars:\nhost_os = \u0026#34;linux\u0026#34; A single variable is declared and are used just for informing which OS we use (Windows or Linux).\noutputs.tf:\noutput \u0026#34;dev_ip\u0026#34; { value = aws_instance.mando_node.public_ip } The only output we have is the public IP address of the EC2 instance, which we\u0026rsquo;ll need to access it.\nuserdata.tpl:\n#!/bin/bash sudo apt -y update \u0026amp;\u0026amp; sudo apt -y install \\ apt-transport-https \\ software-properties-common \\ ca-certificates \\ curl \\ gnupg \\ lsb-release \u0026amp;\u0026amp; sudo mkdir -p /etc/apt/keyrings \u0026amp;\u0026amp; curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg \u0026amp;\u0026amp; echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null \u0026amp;\u0026amp; sudo apt -y update \u0026amp;\u0026amp; sudo apt -y install docker-ce docker-ce-cli containerd.io docker-compose-plugin \u0026amp;\u0026amp; sudo groupadd docker \u0026amp;\u0026amp; newgrp docker \u0026amp;\u0026amp; sudo usermod -aG docker ubuntu This userdata will install docker on the EC2 instance.\nwindows-ssh-config.tpl:\nadd-content -path c:/users/fabio/.ssh/config -value @\u0026#39; Host ${hostname} HostName ${hostname} User ${user} IdentityFile ${identityfile} \u0026#39;@ linux-ssh-config.tpl:\ncat \u0026lt;\u0026lt; EOF \u0026gt;\u0026gt; ~/.ssh/config Host ${hostname} HostName ${hostname} User ${user} IdentityFile ${identityfile} EOF These two template files - windows-ssh-config.tpl and linux-ssh-config.tpl - are used by the local-exec provisioner to add the host information in our ssh configuration. This will make it easier to access it from VS Code.\nAfter you apply this code, you can then proceed to configure Remote SSH on VS Code and use it as a remote dev node. I\u0026rsquo;ll not cover here how to do this since the focus is the Terraform itself, and because it is easily done.\n","date":"30 Jul, 2022","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-aws-dark_hu682fc5fe69b3b8e95c75e446c2abd63e_44442_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-aws-dark_hu682fc5fe69b3b8e95c75e446c2abd63e_44442_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-aws-dark_hu682fc5fe69b3b8e95c75e446c2abd63e_44442_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/terraform/terraform-aws-dark_hu682fc5fe69b3b8e95c75e446c2abd63e_44442_1110x0_resize_q95_h2_box.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/terraform/terraform-aws-dark_hu682fc5fe69b3b8e95c75e446c2abd63e_44442_1110x0_resize_q95_box.jpg\" alt=\"\" width=\"1600\" height=\"498\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/terraform-aws-devenv/","tags":["Terraform","AWS"],"title":"Basic Dev Environment on AWS using Terraform"},{"categories":["Jenkins"],"contents":"After hours and hours of training, videos, documentation, and tutorials, it was time to put into practice the concepts I learned about Jenkins, one of the most used automation tools for CI/CD.\nIt is known that there are many other more modern solutions today, like GitHub Actions or Gitlab CI, for instance, which is a common approach since you can leverage the SCM directly to automate your tasks instead of depending on an external tool. However, Jenkins is still widely adopted and you can perform many automation tasks without much effort using it. It\u0026rsquo;s all going to depend on the situation, the current project, and the team. Traditionally, Jenkins jobs were created using the Jenkins UI and were called FreeStyle jobs. In Jenkins 2.0, a new way was introduced using a technique called pipeline as code, where jobs are created using a script file containing the steps to be executed. That scripted file is called Jenkinsfile. That\u0026rsquo;s just a glimpse on Jenkins pipelines, there\u0026rsquo;s much more depth to it and you can always dive deeper using the official documentation at https://www.jenkins.io/doc/book/pipeline/.\nBefore we start to get into the real stuff, let me just mention a reference that inevitably came into my mind when I first read the name \u0026ldquo;Jenkins\u0026rdquo;, which I\u0026rsquo;m sure that most gamers from early 2000s are gonna remember:\nIf you\u0026rsquo;d like to watch the video, here\u0026rsquo;s the YouTube link to it: Leeroy Jenkins HD 1080p\nAlright, let’s start with the basics.\n1. What\u0026rsquo;s a CI/CD Pipeline? A CI/CD pipeline is usually described as the processes and stages of automation on software delivery. There\u0026rsquo;s usually a specific tool used to achieve this, that orchestrates with SCM and the other tools. It can be used to build code, run unit tests, smoke tests, call external tools like SonarQube, Anchore, Trivy, build Docker images and deploy to environments like Kubernetes or wherever the application is delivered. By performing standardized operations and tests, a CI/CD pipeline can help reduce manual errors, provide feedback to developers, and allow fast iterations.\n2. What\u0026rsquo;s a Jenkinsfile? A Jenkinsfile is just a text file, usually checked in along with the project\u0026rsquo;s source code in an SCM. Ideally, each application will have its Jenkinsfile. A Jenkinsfile can be written in two ways - \u0026ldquo;scripted pipeline syntax\u0026rdquo; or \u0026ldquo;declarative pipeline syntax\u0026rdquo;.\n3. What\u0026rsquo;s a Jenkins Scripted Pipeline? Scripted pipelines run on the Jenkins master with the help of a lightweight executor. It uses very few resources to translate the pipeline into atomic commands. Both declarative and scripted syntax are different from each other and are defined differently. Scripted pipelines are written in Groovy.\nIt requires knowledge of the Groovy language; The Jenkinsfile starts with the word \u0026rsquo;node'; Advanced cabapilities; Groovy engine; Can contain standard programming constructs like if-else block, try-catch block, etc. An example of a Scripted Pipeline:\nnode { stage(\u0026#39;Build\u0026#39;) { if (env.BRANCH_NAME == \u0026#39;master\u0026#39;) { echo \u0026#34;I\u0026#39;m on master brand, building application\u0026#34; bat \u0026#34;msbuild ${C:\\\\Jenkins\\\\my_project\\\\workspace\\\\test\\\\my_project.sln}\u0026#34; } else { echo \u0026#39;Towards the center of the Cloud Computer.\u0026#39; } } stage(\u0026#39;Selenium\u0026#39;) { echo \u0026#39;Running Selenium Smoke Tests\u0026#39; dir(bachelol) { //changes the path to “bachelol” bat \u0026#34;mvn clean test -Dsuite=SMOKE_TEST -Denvironment=QA\u0026#34; } } } 4. What\u0026rsquo;s a Jenkins Declarative Pipeline? The Declarative Pipeline is relatively new and provides a simplified, opinionated syntax on top of the Pipeline subsystems. Its syntax offers an easy way to create pipelines. It contains a predefined hierarchy and gives you the ability to control all aspects of a pipeline execution in a simple, straightforward manner.\nThe latest addition in Jenkins pipeline job creation technique; Needs to use the predefined constructs to create pipelines; Hence, it is not flexible as a scripted pipeline; The Jenkinsfile starts with the word \u0026lsquo;pipeline\u0026rsquo;, and there is a predefined structure; Usually, the preferred way to start, as they offer a rich set of features, come with less learning curve \u0026amp; no prerequisite to learn a programming language like Groovy just for the sake of writing pipeline code; We can also validate the syntax of the Declarative pipeline code before running the job. It helps to avoid a lot of runtime issues with the build script. 5. Why run Jenkins on Docker? Despite the inherent advantages (and caveats) of running anything in Docker, the main reasons I could think of are:\nAbility to keep the server configuration under version control; Run multiple copies of the server anywhere; Integrate with Kubernetes and other orchestration platforms; Jenkins\u0026rsquo; official Docker image is widely adopted and maintained; Simple implementation = simple administration. The main (Drawback|Caveat|Problem|Issue|Flaw) is when you want to use agents with docker too. To do that you\u0026rsquo;re gonna need some implementation of Docker in Docker. There are a few ways to do that, but the two easiest ways are connecting the Jenkins container directly to the host\u0026rsquo;s Docker socket using -v /var/run/docker.sock:/var/run/docker.sock and changing the permissions, or enabling TCP on the Docker server-side. If you can run your Jenkins server on Kubernetes, you can enable it to launch new agent pods inside the cluster which gives you even more flexibility. I\u0026rsquo;ll try that in the future since I\u0026rsquo;m out of credits on AWS and my dev cluster is behind a proxy, which is a real pain to deal with.\n6. Building a Jenkins Image The Dockerfile below creates an image based on the latest Jenkins LTS from Dockerhub, and preinstalls the plugins needed:\nFROM jenkins/jenkins:lts # Skip setup wizard ENV JAVA_OPTS=\u0026#34;-Djenkins.install.runSetupWizard=false\u0026#34; # Set docker group ARG DOCKER_GID=998 # Get plugins RUN /usr/local/bin/install-plugins.sh \\ workflow-multibranch:latest \\ pipeline-model-definition:latest \\ pipeline-stage-view:latest \\ git:latest \\ credentials:latest \\ slack:latest # Install Docker \u0026amp; docker-compose USER root RUN apt-get update \u0026amp;\u0026amp; \\ apt-get -y install apt-transport-https \\ ca-certificates \\ curl \\ gnupg2 \\ software-properties-common \u0026amp;\u0026amp; \\ curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo \u0026#34;$ID\u0026#34;)/gpg \u0026gt; /tmp/dkey; apt-key add /tmp/dkey \u0026amp;\u0026amp; \\ add-apt-repository \\ \u0026#34;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo \u0026#34;$ID\u0026#34;) \\ $(lsb_release -cs) \\ stable\u0026#34; \u0026amp;\u0026amp; \\ apt-get update \u0026amp;\u0026amp; \\ groupadd -g ${DOCKER_GID} docker \u0026amp;\u0026amp; \\ apt-get -y install docker-ce docker-ce-cli containerd.io RUN curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/local/bin/docker-compose RUN usermod -aG docker jenkins USER jenkins I referred to the official documentation: https://docs.docker.com/compose/install/ https://docs.docker.com/engine/install/ubuntu/\nTo spin up a Jenkins instance on my local machine, I used a docker-compose.yml file and put everything on the same folder. The jenkins service points to a dockerfile on the same path, and the container will be accessible on localhost:9080. I mapped the docker.sock to the container and matched the docker GID with the host, and also created and mapped a jenkins_home folder locally to persist data:\nversion: \u0026#34;3.8\u0026#34; services: jenkins: build: context: . dockerfile: Dockerfile ports: - \u0026#34;127.0.0.1:9080:8080\u0026#34; volumes: - ./jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped Now that we have a running Jenkins instance with the plugins installed and with access to the host\u0026rsquo;s dockerd, let\u0026rsquo;s start using it to automate a build.\n7. The Spring PetClinic Sample Application To test everything, I downloaded the \u0026ldquo;Spring PetClinic Sample Application\u0026rdquo; (can be found here: https://github.com/spring-projects/spring-petclinic), which is written in Java using Spring Boot and built with Maven. If you clone the official repository, you\u0026rsquo;ll get a Maven Wrapper bundled together that can be used to build the application. So before trying to automate anything with Jenkins, let\u0026rsquo;s first build, test, and run the application manually:\n# Clone the repo git clone https://github.com/spring-projects/spring-petclinic.git # Enter the directory cd spring-petclinic #Build the application ./mvnw package #Run the application java -jar target/*.jar The first time I ran the mvnw package command, it took a while to finish since it downloaded all the dependencies. After that, subsequent runs took around a minute to finish. That\u0026rsquo;s pretty much all we have to do to test if it builds and runs properly, I just followed the instructions from the repository without changing anything. If it runs, we know that the code should work inside the pipeline too. The changes will come when we use the Maven from Jenkins and try to run JUnit tests.\nOutput from the build:\n[INFO] Building jar: ./github/spring-petclinic/target/spring-petclinic-2.5.0-SNAPSHOT.jar [INFO] [INFO] --- spring-boot-maven-plugin:2.5.4:repackage (repackage) @ spring-petclinic --- [INFO] Replacing main artifact with repackaged archive [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 01:15 min [INFO] Finished at: 2021-10-19T11:35:43-03:00 [INFO] ------------------------------------------------------------------------ Output from java -jar target/*.jar:\njava -jar target/*.jar |\\ _,,,--,,_ /,`.-\u0026#39;`\u0026#39; ._ \\-;;,_ _______ __|,4- ) )_ .;.(__`\u0026#39;-\u0026#39;__ ___ __ _ ___ _______ | | \u0026#39;---\u0026#39;\u0026#39;(_/._)-\u0026#39;(_\\_) | | | | | | | | | | _ | ___|_ _| | | | | |_| | | | __ _ _ | |_| | |___ | | | | | | | | | | \\ \\ \\ \\ | ___| ___| | | | _| |___| | _ | | _| \\ \\ \\ \\ | | | |___ | | | |_| | | | | | | |_ ) ) ) ) |___| |_______| |___| |_______|_______|___|_| |__|___|_______| / / / / ==================================================================/_/_/_/ :: Built with Spring Boot :: 2.5.4 2021-10-19 11:38:31.201 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : Starting PetClinicApplication v2.5.0-SNAPSHOT using Java 11.0.11 on my-machine with PID 39806 (./github/spring-petclinic/target/spring-petclinic-2.5.0-SNAPSHOT.jar started by biofa in ./github/spring-petclinic) 2021-10-19 11:38:31.204 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : No active profile set, falling back to default profiles: default 2021-10-19 11:38:32.230 INFO 39806 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode. 2021-10-19 11:38:32.288 INFO 39806 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 50 ms. Found 4 JPA repository interfaces. 2021-10-19 11:38:33.020 INFO 39806 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2021-10-19 11:38:33.036 INFO 39806 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2021-10-19 11:38:33.037 INFO 39806 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.52] 2021-10-19 11:38:33.128 INFO 39806 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2021-10-19 11:38:33.128 INFO 39806 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1865 ms 2021-10-19 11:38:33.377 INFO 39806 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting... 2021-10-19 11:38:33.590 INFO 39806 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 2021-10-19 11:38:33.919 INFO 39806 --- [ main] org.ehcache.core.EhcacheManager : Cache \u0026#39;vets\u0026#39; created in EhcacheManager. 2021-10-19 11:38:33.934 INFO 39806 --- [ main] org.ehcache.jsr107.Eh107CacheManager : Registering Ehcache MBean javax.cache:type=CacheStatistics,CacheManager=urn.X-ehcache.jsr107-default-config,Cache=vets 2021-10-19 11:38:33.941 INFO 39806 --- [ main] org.ehcache.jsr107.Eh107CacheManager : Registering Ehcache MBean javax.cache:type=CacheStatistics,CacheManager=urn.X-ehcache.jsr107-default-config,Cache=vets 2021-10-19 11:38:34.021 INFO 39806 --- [ main] o.hibernate.jpa.internal.util.LogHelper : HHH000204: Processing PersistenceUnitInfo [name: default] 2021-10-19 11:38:34.081 INFO 39806 --- [ main] org.hibernate.Version : HHH000412: Hibernate ORM core version 5.4.32.Final 2021-10-19 11:38:34.204 INFO 39806 --- [ main] o.hibernate.annotations.common.Version : HCANN000001: Hibernate Commons Annotations {5.1.2.Final} 2021-10-19 11:38:34.332 INFO 39806 --- [ main] org.hibernate.dialect.Dialect : HHH000400: Using dialect: org.hibernate.dialect.H2Dialect 2021-10-19 11:38:34.955 INFO 39806 --- [ main] o.h.e.t.j.p.i.JtaPlatformInitiator : HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform] 2021-10-19 11:38:34.962 INFO 39806 --- [ main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit \u0026#39;default\u0026#39; 2021-10-19 11:38:36.371 INFO 39806 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 13 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; 2021-10-19 11:38:36.441 INFO 39806 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; 2021-10-19 11:38:36.453 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : Started PetClinicApplication in 5.753 seconds (JVM running for 6.136) 8. Using Maven on Jenkins There are a couple of different ways to use Maven with Jenkins. There is a Maven plugin for Jenkins, and we can install Maven on the Jenkins server and set it up at \u0026ldquo;Global Tool Configuration\u0026rdquo;. Since we want to use docker containers as executors or agents, that would not be an option. We could also use the wrapper that comes with the project, but a better and simple route would be to use the official Maven docker image, and that way we can have an agent always updated and completely separate Maven from our Jenkins server. This approach makes it easier to manage the environment and also allows for a single Jenkins server to work with different types of builds with a single installation, without having to embed every tool used on the server.\nI built a pipeline with six stages: Init, Build, Test and Build Image, Publish Image and Prod. Some of the main aspects are:\nIt mounts the m2 directory, which is used by maven, locally to have cache between builds, and also settings.xml to enable setting custom parameters to Maven; Maven 3.8.3 with AdoptOpenJDK 11 Docker Image is used as an agent only for the stages that require it; Dockerhub credentials were added previously on Jenkins. pipeline { agent none environment { DEPLOY_TO = \u0026#39;staging\u0026#39; imageName = \u0026#34;fabioctba/spring-petclinic\u0026#34; registryCredential = \u0026#39;dockerhub\u0026#39; dockerImage = \u0026#39;\u0026#39; } stages { stage(\u0026#39;Init\u0026#39;){ agent any steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Starting\u0026#34; sh \u0026#39;git --version\u0026#39; echo \u0026#34;Deploying to ${DEPLOY_TO}\u0026#34; } } stage(\u0026#39;Build\u0026#39;) { agent { docker { image \u0026#39;maven:3.8.3-adoptopenjdk-11\u0026#39; args \u0026#39;-v m2:/root/.m2 -v settings.xml:/root/.m2/settings.xml\u0026#39; } } steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Building\u0026#34; sh \u0026#39;mvn -B -DskipTests clean package\u0026#39; } } stage(\u0026#39;Test\u0026#39;) { agent { docker { image \u0026#39;maven:3.8.3-adoptopenjdk-11\u0026#39; args \u0026#39;-v m2:/root/.m2 -v settings.xml:/root/.m2/settings.xml\u0026#39; } } steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Running Tests\u0026#34; sh \u0026#39;mvn test\u0026#39; } post { always { archiveArtifacts artifacts: \u0026#39;target/*.jar\u0026#39; junit \u0026#39;target/surefire-reports/*.xml\u0026#39; } } } stage(\u0026#39;Build Image\u0026#39;) { agent any steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Building Image\u0026#34; script { dockerImage = docker.build imageName } } } stage(\u0026#39;Publish Image\u0026#39;) { agent any steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Building Image\u0026#34; script { docker.withRegistry( \u0026#39;\u0026#39;, registryCredential ) { dockerImage.push(\u0026#34;$BUILD_NUMBER\u0026#34;) dockerImage.push(\u0026#39;latest\u0026#39;) } } } } stage(\u0026#39;Prod\u0026#39;) { agent any when { allOf { branch \u0026#39;master\u0026#39;; environment name: \u0026#39;DEPLOY_TO\u0026#39;, value: \u0026#39;production\u0026#39; } } steps { slackSend channel: \u0026#39;#jenkins\u0026#39;, message: \u0026#34;${env.BUILD_ID} on ${env.JENKINS_URL} - Running Production stage\u0026#34; echo \u0026#39;Some extra step when on production release\u0026#39; } } } } I used the Slack Plugin to integrate with Jenkins, following this documentation. I used only two variables just to illustrate what can be sent as a message, but a full list of environment variables can be found directly on Jenkins accessing http://localhost:9080/env-vars.html. Of course, you have to change localhost:9080 to your server\u0026rsquo;s address.\nThe Init stage prints the versions of the main tools used, useful to debug problems;\nThe Build stage runs mvn package, which creates the jar files;\nThe Test stage run the Unit tests. Spring Pet Clinic has a total of 40 tests configured, and that would depend on how the dev team created their tests;\nThe Build Image stage creates a Docker image and publishes it to the Dockerhub;\nThe Publish Image stage publishes the image to Dockerhub;\nThe Prod stage doesn\u0026rsquo;t do anything, it\u0026rsquo;s just an example of a conditional step that can be executed depending on a variable.\nNotice that there isn\u0026rsquo;t a stage to clone the repository at the beginning; this stage is implicit when you configure Jenkins to pull the pipeline from SCM, which is the plan here.\nHere\u0026rsquo;s the Dockerfile I used to publish the application:\nFROM openjdk:11-jre-slim RUN groupadd --gid 1000 java \\ \u0026amp;\u0026amp; useradd --uid 1000 --gid java --shell /bin/bash --create-home java USER java VOLUME /tmp WORKDIR /app COPY --chown=java:java ./target/*.jar /app/ CMD [\u0026#34;java\u0026#34;,\u0026#34;-Djava.security.egd=file:/dev/./urandom\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app/*.jar\u0026#34;] That is a working pipeline, and I wanted to include most of the concepts I saw in the courses. There are other features I\u0026rsquo;d like to try in the future like shared libraries and multi-branch pipelines, but this might go into a different post.\n9. Courses I either took or recommend: a. Jenkins, From Zero To Hero: Become a DevOps Jenkins Master Udemy, by Ricardo Andre Gonzalez Gomez, 2018 https://www.udemy.com/course/jenkins-from-zero-to-hero/\nI found this course a little outdated and looks like the author stopped maintaining it in 2020. However, if you never used Jenkins before and want a cheap course to start, I strongly recommend this one. It\u0026rsquo;s also well recommended in many other sources and has a high rate at Udemy. Ricardo focuses a lot on Docker, so it might be better targeted to someone without any experience in Docker also.\nb. Continuous Integration with Jenkins Pluralsight, by different authors, updated constantly https://app.pluralsight.com/paths/skill/continuous-integration-with-jenkins\nPluralsight has a total of 8 courses, each with around 2 hours of duration, which makes the Jenkins Skill Path. I took four of them during the last Pluralsight Free Week and like other courses I took from Pluralsight, the quality was much superior. Unfortunately, Pluralsight costs a great amount of money, so I usually stay on the lookout for their periodically free weeks and free weekends. The good thing is that you can download the course material and most of the authors have GitHub repositories where you can check the code later.\n10. References https://stackoverflow.com/questions/44440164/what-are-the-advantages-of-running-jenkins-in-a-docker-container\nhttps://www.cinqict.nl/blog/building-a-jenkins-development-docker-image\nhttps://tomgregory.com/building-a-spring-boot-application-in-jenkins/\nIf you read it to the end, I\u0026rsquo;d like to hear your comments, and hope this helped you in some way. Cheers.\nAnd sorry for the long post\u0026hellip; here\u0026rsquo;s a potato.\nSource: @truth.potato\n","date":"18 Oct, 2021","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cimg src=\"/images/jenkins/Jenkins-logo-001.svg\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"w-100 img-fluid rounded\" width=\"\" height=\"\"\u003e\r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/jenkins-1/","tags":["Jenkins"],"title":"Getting started with Jenkins Declarative Pipelines"},{"categories":["Ceph","Kubernetes"],"contents":"I recently got 10 bare-metal servers to play with, they used to be part of our first Ceph cluster and got replaced with more powerful hardware. So i built two k8s clusters and decided to give Rook a try.\nAs the cluster grew bigger, we purchased not only more servers but with different configuration. But those Dell R530 servers still have pretty decent power to run many internal demands we have, so i built a Ceph cluster with four of them using CentOS 8, Ceph Octopus and deployed everything in containers. But i want to talk about what i did with the remaining six servers, that became two kubernetes clusters - one with Flatcar and the other with Centos, to replicate some production scenarios we have. Since the servers have 8 2TB HDD each and we use just one for the OS, that would be a perfect scenario to experiment with Rook. I tinkered with it previously with some labs but now i could really test its usability.\nGetting Rook to run is pretty straightforward, it might not even be necessary to have great knowledge about ceph for the initial setup. However it would be mandatory to know about Ceph administration for a production environment. There are many administrative tasks that gets taken care of by Rook, but after working with Ceph for the past 3 years i can assure you that there will be many situations where the sysadmin will have to step up.\nI used the quickstart guide as a reference, it can be found on the Rook GitHub page: https://rook.github.io/docs/rook/v1.6/ceph-quickstart.html.\nThe only parameter i changed from the defaults was the ROOK_ENABLE_DISCOVERY_DAEMON, since i wanted it to automatically detect and provision the OSDs on every empty device. It\u0026rsquo;s important to erase the disks prior to the deployment, because it will skip any device with partitions. For that you can use wipefs or even dd. Be aware that if the device has LVM partitions and you want to avoid a reboot you might want to delete the LVs and VGs first. If you wipe the device the kernel might not release the mapped lvm volumes and you will not be able to delete after that. Needless to say that i made that mistake a few times before learning the lesson.\nCommands i used:\ngit clone --single-branch --branch master https://github.com/rook/rook.git git checkout release-1.6 cd rook/cluster/examples/kubernetes/ceph sed -i s/\u0026#34;ROOK_ENABLE_DISCOVERY_DAEMON: \\\u0026#34;false\\\u0026#34;\u0026#34;/\u0026#34;ROOK_ENABLE_DISCOVERY_DAEMON: \\\u0026#34;true\\\u0026#34;\u0026#34;/ operator.yaml kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml Checking if the pods were created:\n$ kubectl -n rook-ceph get pod NAME READY STATUS RESTARTS AGE csi-cephfsplugin-7tkgk 3/3 Running 25 95d csi-cephfsplugin-cv2kq 3/3 Running 6 95d csi-cephfsplugin-provisioner-bc5cff84-8h5b4 6/6 Running 19 95d csi-cephfsplugin-provisioner-bc5cff84-cb477 6/6 Running 6 95d csi-cephfsplugin-qvqjc 3/3 Running 3 95d csi-rbdplugin-2rqwk 3/3 Running 25 95d csi-rbdplugin-g5tkj 3/3 Running 3 95d csi-rbdplugin-provisioner-97957587f-gvn6r 6/6 Running 0 22d csi-rbdplugin-provisioner-97957587f-lflc7 6/6 Running 8 95d csi-rbdplugin-vcsdj 3/3 Running 6 95d rook-ceph-crashcollector-node1-7fcbcd7dc6-6fbq6 1/1 Running 2 73d rook-ceph-crashcollector-node2-7f699c88c-78gvw 1/1 Running 0 22d rook-ceph-crashcollector-node3-77777995d8-dtsrh 1/1 Running 1 95d rook-ceph-mgr-a-8486cbdf64-dsdn2 1/1 Running 0 24d rook-ceph-mon-a-7758d4d54c-crq4s 1/1 Running 2 95d rook-ceph-mon-d-77db79f9b9-fcd5r 1/1 Running 0 41d rook-ceph-mon-e-7d44dcff6b-zjpfl 1/1 Running 0 22d rook-ceph-operator-66f7668857-sr8bw 1/1 Running 2 95d rook-ceph-osd-0-57b7d8b47b-k5ps8 1/1 Running 2 95d rook-ceph-osd-1-548c7bc54d-zsp8r 1/1 Running 1 95d rook-ceph-osd-10-7875d885d8-h2wdt 1/1 Running 0 22d rook-ceph-osd-11-5585fb7856-gxjfx 1/1 Running 2 95d rook-ceph-osd-12-fd5bcb8f8-z9v4c 1/1 Running 1 95d rook-ceph-osd-13-784c797d46-qmxj4 1/1 Running 0 22d rook-ceph-osd-14-6b689bfb4-sfffq 1/1 Running 2 95d rook-ceph-osd-15-64984bf7db-h99q2 1/1 Running 1 95d rook-ceph-osd-16-6f66f68868-ft8d8 1/1 Running 0 22d rook-ceph-osd-17-66fcdfc8c8-rg76t 1/1 Running 2 95d rook-ceph-osd-18-85f9d5567b-6jjph 1/1 Running 1 95d rook-ceph-osd-19-969b58fb-ztzxx 1/1 Running 0 22d rook-ceph-osd-2-ffc69957b-846mj 1/1 Running 2 95d rook-ceph-osd-20-5c86c5d556-26kkp 1/1 Running 0 22d rook-ceph-osd-3-79cb4b8f78-wxczm 1/1 Running 1 95d rook-ceph-osd-4-9587d6994-z94l2 1/1 Running 0 22d rook-ceph-osd-5-74cb556886-sqp6g 1/1 Running 2 95d rook-ceph-osd-6-5fb96bcb-2lgqr 1/1 Running 1 95d rook-ceph-osd-7-85f8659c9d-7z5tn 1/1 Running 0 22d rook-ceph-osd-8-6765676bfb-kjxr5 1/1 Running 2 95d rook-ceph-osd-9-67b4f98cdb-h9bd8 1/1 Running 1 95d rook-ceph-osd-prepare-node1-6x7dr 0/1 Completed 0 133m rook-ceph-osd-prepare-node2-z6jl8 0/1 Completed 0 133m rook-ceph-osd-prepare-node3-rvhth 0/1 Completed 0 133m rook-ceph-tools-6f58686b5d-znnsg 1/1 Running 0 24d rook-discover-2684s 1/1 Running 7 95d rook-discover-2nlkj 1/1 Running 1 95d rook-discover-4rhxm 1/1 Running 2 95d The output is long but i wanted to show that Rook actually created a OSD por for every device in each node, and i configured kubernetes to allow that on the master node either.\nWe can see that we now have a bunch of new resource definitions, enabling us to interact with Ceph in a declarative way, instead of recurring to the CLI to manage users, filesystems, pools and many other resources:\n$ kubectl get crd | grep \u0026#39;rook\\|objectbucket\u0026#39; cephblockpools.ceph.rook.io 2021-04-01T19:30:54Z cephclients.ceph.rook.io 2021-04-01T19:30:53Z cephclusters.ceph.rook.io 2021-04-01T19:30:53Z cephfilesystems.ceph.rook.io 2021-04-01T19:30:53Z cephnfses.ceph.rook.io 2021-04-01T19:30:53Z cephobjectrealms.ceph.rook.io 2021-04-01T19:30:54Z cephobjectstores.ceph.rook.io 2021-04-01T19:30:53Z cephobjectstoreusers.ceph.rook.io 2021-04-01T19:30:54Z cephobjectzonegroups.ceph.rook.io 2021-04-01T19:30:54Z cephobjectzones.ceph.rook.io 2021-04-01T19:30:54Z cephrbdmirrors.ceph.rook.io 2021-04-01T19:30:53Z objectbucketclaims.objectbucket.io 2021-04-01T19:30:54Z objectbuckets.objectbucket.io 2021-04-01T19:30:54Z volumes.rook.io 2021-04-01T19:30:54Z From that point we can access the Ceph CLI by interacting with the rook-ceph-tools pod:\n$ kubectl exec -it -n rook-ceph rook-ceph-tools-6f58686b5d-znnsg -- ceph status cluster: id: 72e9c4a9-4315-45e0-ad43-cd79d22fb2be health: HEALTH_OK services: mon: 3 daemons, quorum a,d,e (age 7d) mgr: a(active, since 3w) osd: 21 osds: 21 up (since 3w), 21 in (since 3w) rgw: 1 daemon active (my.store.a) task status: data: pools: 30 pools, 401 pgs objects: 2.12k objects, 2.4 GiB usage: 29 GiB used, 38 TiB / 38 TiB avail pgs: 401 active+clean io: client: 7.0 KiB/s rd, 72 KiB/s wr, 6 op/s rd, 16 op/s wr Now we have two new StorageClasses to work with:\n$ kubectl get storageclasses\rNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE\rrook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 64d\rrook-ceph-bucket rook-ceph.ceph.rook.io/bucket Delete Immediate false 84d And then we provisioned some volumes to a few applications running on the cluster:\n$ kubectl get pv -A\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-1c9151ae-3e9e-49ae-adbf-f365de4eb718 8Gi RWO Delete Bound cluster-spd/data-etcd-1 rook-ceph-block 18d\rpvc-39c5a513-1c50-48a9-9e4d-354ff84ea8b4 8Gi RWO Delete Bound cluster-spb/data-etcd-0 rook-ceph-block 24d\rpvc-53f2816a-e4c2-434d-8600-20206d3f6f75 8Gi RWO Delete Bound cluster-spc/data-etcd-0 rook-ceph-block 18d\rpvc-553c1a1b-0131-44a9-8a67-293dea40c161 5Gi RWO Delete Bound gitlab/redis-data-gitlab-redis-master-0 rook-ceph-block 56d\rpvc-6c38a9b7-8e23-4478-a431-34fd4f9ec158 8Gi RWO Delete Bound cluster-spd/data-etcd-2 rook-ceph-block 18d\rpvc-76bfab8e-6ae6-4b11-8223-ea9e97bbccf4 8Gi RWO Delete Bound cluster-spc/data-etcd-2 rook-ceph-block 18d\rpvc-836c1ff1-c92d-467c-b45b-b970824e2b04 8Gi RWO Delete Bound cluster-spc/data-etcd-1 rook-ceph-block 18d\rpvc-87e7bd8e-250e-4bd4-8268-3fb49597d0ab 10Gi RWO Delete Bound gitlab/gitlab-minio rook-ceph-block 39d\rpvc-bac3dc77-73ae-4800-821a-04faa15e627d 8Gi RWO Delete Bound cluster-spb/data-etcd-1 rook-ceph-block 24d\rpvc-ca087b82-038b-418a-a187-8e28617fbfae 8Gi RWO Delete Bound cluster-spd/data-etcd-0 rook-ceph-block 18d\rpvc-d255ca76-b62f-413b-82b8-184ff3cb0026 8Gi RWO Delete Bound cluster-spb/data-etcd-2 rook-ceph-block 24d\rpvc-d331d766-72d7-46c4-9cba-c0e8fea7a2b8 50Gi RWO Delete Bound gitlab/repo-data-gitlab-gitaly-0 rook-ceph-block 56d\rpvc-e76d375b-03b5-4bc0-8725-bc406cb7ee94 8Gi RWO Delete Bound gitlab/data-gitlab-postgresql-0 rook-ceph-block 56d This is how the pvc manifest looks like:\n$ kubectl get pvc -n gitlab gitlab-minio -o yaml\rapiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: gitlab-minio\rnamespace: gitlab\rlabels:\rapp: minio\rspec:\rstorageClassName: rook-ceph-block\raccessModes:\r- ReadWriteOnce\rresources:\rrequests:\rstorage: 10Gi We can also provision Buckets using S3 via Rook. We used this to provide a backend for a Hashicorp Vault we have on the same environment:\nFirst we have to create the StorageClass:\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: rook-ceph-bucket\rprovisioner: rook-ceph.ceph.rook.io/bucket\rreclaimPolicy: Delete\rparameters:\robjectStoreName: my-store\robjectStoreNamespace: rook-ceph\rregion: us-east-1 Then we create a ObjectBucketClaim, which is a new resource type defined by the Rook CRD:\napiVersion: objectbucket.io/v1alpha1\rkind: ObjectBucketClaim\rmetadata:\rname: ceph-bucket\rnamespace: vault\rspec:\rgenerateBucketName: ceph-bucket\rstorageClassName: rook-ceph-bucket ","date":"17 Jul, 2021","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/ceph/rook_hu3357ff1ce9c006764e4a15dc23e2cc02_44755_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/rook_hu3357ff1ce9c006764e4a15dc23e2cc02_44755_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/rook_hu3357ff1ce9c006764e4a15dc23e2cc02_44755_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/rook_hu3357ff1ce9c006764e4a15dc23e2cc02_44755_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/ceph/rook_hu3357ff1ce9c006764e4a15dc23e2cc02_44755_1110x0_resize_box_3.png\" alt=\"\" width=\"982\" height=\"296\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/rook/","tags":["Ceph","Kubernetes"],"title":"Using Rook to leverage Ceph storage on a Kubernetes cluster"},{"categories":["Python"],"contents":"So I needed to learn how to scrape a web page using Python. My teammate suggested that I could learn a few tricks in python by scraping FIIs, which is the brazilian equivalent to american REITs. I did some research and found some good examples using the module Pandas, a tool designed for data analisys and manipulation and very popular among people working with data science. More information can be found here: Reading and Writing CSV Files in Python My goal was much less ambitious but I thought I might as well use what everyone else is using, since it would provide a good learning opportunity.\nI based my code on the example from Renata Magner\u0026rsquo;s GitHub. I still have to implement error handling and many other improvements on the code, but that\u0026rsquo;s just the first version. Without further ado, here\u0026rsquo;s what I came up with:\nfrom bs4 import BeautifulSoup import requests import pandas as pd url = \u0026#39;https://www.fundsexplorer.com.br/ranking\u0026#39; html = requests.get(url).content soup = BeautifulSoup(html, \u0026#39;html.parser\u0026#39;) # stores scraped data on a list results = soup.find_all(\u0026#34;td\u0026#34;) # counts how many entries we got. each entry has 26 \u0026#39;td\u0026#39; lines each. fii_count=int(len(results)/26) # initialize the variables fii_info={} fii_list=[] \u0026#39;\u0026#39;\u0026#39; index - field [0] - COD FUNDO [1] - SETOR [2] - PRECO_ATUAL [3] - LIQUIDEZ DIARIA [4] - DIVIDENDO [5] - dividend_yield [6] - dy_3m [7] - dy_6m [8] - dy_12m [9]- dy_3m_media [10] - dy_6m_media [11] - dy_12m_media [12] - dy_ano [13] - variacao_preco [14] - rentabilidade_periodo [15] - rentabilidade_acumulada [16] - patrimonio_liq [17] - vpa [18] - p_vpa \u0026#39;\u0026#39;\u0026#39; # runs through collected data and extracts the desided info for fii in range(fii_count): fii_info[\u0026#39;codigo_fundo\u0026#39;]=results[fii*26].get_text() fii_info[\u0026#39;preco_atual\u0026#39;]=results[fii*26+2].get_text() fii_info[\u0026#39;setor\u0026#39;]=results[fii*26+1].get_text() fii_info[\u0026#39;p/vpa\u0026#39;]= results[fii*26+18].get_text() fii_info[\u0026#39;dividend_yield\u0026#39;]=results[fii*26+5].get_text() fii_info[\u0026#39;dividendo\u0026#39;]=results[fii*26+4].get_text() fii_info[\u0026#39;dy_12m_media\u0026#39;]=results[fii*26+11].get_text() fii_list.append(fii_info) fii_info={} # generates the CSV output fii_table = pd.DataFrame(data=fii_list) fii_table.to_csv(\u0026#39;fii_table.csv\u0026#39;) References:\nhttps://github.com/RenataMagner/web_scraping_fii/blob/master/web_scraping-fiiv2.ipynb\nhttps://realpython.com/python-csv/\nhttps://www.fundsexplorer.com.br/ranking\nThanks for reading!\n","date":"20 Nov, 2020","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/python/python-beautifulsoup_hu5e5e8c6dc6d151763d2af408b87dbc48_104862_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python-beautifulsoup_hu5e5e8c6dc6d151763d2af408b87dbc48_104862_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python-beautifulsoup_hu5e5e8c6dc6d151763d2af408b87dbc48_104862_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python-beautifulsoup_hu5e5e8c6dc6d151763d2af408b87dbc48_104862_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/python/python-beautifulsoup_hu5e5e8c6dc6d151763d2af408b87dbc48_104862_1110x0_resize_box_3.png\" alt=\"\" width=\"1280\" height=\"479\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/python-web-scrapper-fii/","tags":["Python"],"title":"Web scrapper using Python"},{"categories":["Python"],"contents":"So I needed to learn how to use decorators in Python.\nI tried to come up with something within my current knowledge and since I was studying sorting algorithms, I decided to use decorators to measure how long each algorithm would take to complete and compare them. I know that it is kind of obvious which one is better in the proposed scenario for anyone with a Bachelor in Computer Science or any experienced developer, but the purpose here was to learn and develop my python skills.\rDecorators can be very useful when you need to add functionalities to an existing function/method/class, without having to change its code. Let\u0026rsquo;s say you need to add an additional check, time the function or log but you are using a module instead of writing your own function, or you want to reuse the code more efficiently, then using decorators might be the way to go.\nThere are many other ways to use decorators that i still have to explore, like chained decorators for instance.\nI found a good post at https://kleiber.me/blog/2017/08/10/tutorial-decorator-primer/ , where the author uses the same example I used (comparing sorting algorithms), but he used the pygorithm.sorting module, while I used my own functions.\nThere are many different ways to approach this, and we can always improve our older codes as we get better at a programming language. With that in mind, here\u0026rsquo;s the code i came up with, using Python 3:\nimport random import functools import time # define the function to measure time def timeIt(func): # wraps the function using functools @functools.wraps(func) def newfunc(*args, **kwargs): # check to enter the function only once if not hasattr(newfunc, \u0026#39;_entered\u0026#39;): newfunc._entered = True # starts the measurement startTime = time.time() func(*args, **kwargs) # finishes the measurement elapsedTime = time.time() - startTime # prints the result print(\u0026#39;function [{}] finished in {} ms\u0026#39;.format( func.__name__, int(elapsedTime * 1000))) del newfunc._entered return newfunc # the @ inserts decorator @timeIt def mergeSort(L): if len(L) \u0026gt; 1: mid = len(L) // 2 left = L[:mid] right = L[mid:] mergeSort(left) mergeSort(right) i = j = k = 0 while i \u0026lt; len(left) and j \u0026lt; len(right): if left[i] \u0026lt; right[j]: L[k] = left[i] i += 1 else: L[k] = right[j] j += 1 k += 1 while i \u0026lt; len(left): L[k] = left[i] i += 1 k += 1 while j \u0026lt; len(right): L[k] = right[j] j += 1 k += 1 @timeIt def selectionSort(L): for i in range(0, len(L)): min_i = i for right in range(i + 1, len(L)): if L[right] \u0026lt; L[min_i]: min_i = right L[i], L[min_i] = L[min_i], L[i] @timeIt def bubbleSort(L): elem = len(L) - 1 issorted = False while not issorted: issorted = True for i in range(elem): if L[i] \u0026gt; L[i + 1]: L[i], L[i + 1] = L[i + 1], L[i] issorted = False randomList = random.sample(range(5000), 5000) mergeSort(randomList.copy()) selectionSort(randomList.copy()) bubbleSort(randomList.copy()) And the results:\nfunction [mergeSort] finished in 2 ms function [selectionSort] finished in 930 ms function [bubbleSort] finished in 2828 ms ecco!\nReferences:\nhttps://docs.python.org/3/library/timeit.html\nhttps://stackoverflow.com/questions/5478351/python-time-measure-function\n","date":"17 Nov, 2020","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/python/python_banner_hu0dd6cdd0259727391bbd20aedc813b79_113384_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python_banner_hu0dd6cdd0259727391bbd20aedc813b79_113384_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python_banner_hu0dd6cdd0259727391bbd20aedc813b79_113384_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/python/python_banner_hu0dd6cdd0259727391bbd20aedc813b79_113384_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/python/python_banner_hu0dd6cdd0259727391bbd20aedc813b79_113384_1110x0_resize_box_3.png\" alt=\"\" width=\"1465\" height=\"434\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/python-sort-comparsion/","tags":["Python"],"title":"Comparsion of Sorting Algorithms using Python Decorators"},{"categories":["Ceph"],"contents":"While working on an upgrade on one of our ceph cluster from Luminous to Nautilus, I needed to come up with a way to detect any client with older versions, and check if we could break anything after the upgrade.\nI started checking the documentation as always, but all I found was a command called ceph features, which gives a summarized output:\n[root@mon-1 ~]# ceph features { \u0026#34;mon\u0026#34;: { \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x3ffddff8eeacfffb\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;luminous\u0026#34;, \u0026#34;num\u0026#34;: 3 } }, \u0026#34;mds\u0026#34;: { \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x3ffddff8eeacfffb\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;luminous\u0026#34;, \u0026#34;num\u0026#34;: 3 } }, \u0026#34;osd\u0026#34;: { \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x3ffddff8eeacfffb\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;luminous\u0026#34;, \u0026#34;num\u0026#34;: 192 } }, \u0026#34;client\u0026#34;: { \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x27018fb86aa42ada\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;jewel\u0026#34;, \u0026#34;num\u0026#34;: 422 }, \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x2f018fb86aa42ada\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;luminous\u0026#34;, \u0026#34;num\u0026#34;: 95 }, \u0026#34;group\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x3ffddff8eeacfffb\u0026#34;, \u0026#34;release\u0026#34;: \u0026#34;luminous\u0026#34;, \u0026#34;num\u0026#34;: 18 } } } So that got me worried, since I was now looking at 422 clients apparently using jewel and we were already using Nautilus for a while. Since this specific cluster was \u0026ldquo;born\u0026rdquo; in jewel, I thought that it might be the case that many clients didn\u0026rsquo;t get any upgrades since.\nAnother way to see who\u0026rsquo;s connected to the cluster is the command ceph tell mds.\\hostname` client ls`, but it shows only cephfs clients, which solves only a part of the problem. At least we can see the client\u0026rsquo;s IP address (adresses and names omitted with [\u0026hellip;]):\n\u0026#34;inst\u0026#34;: \u0026#34;client.1400410 v1:[...]]:0/1769731892\u0026#34;, \u0026#34;completed_requests\u0026#34;: [], \u0026#34;prealloc_inos\u0026#34;: [], \u0026#34;used_inos\u0026#34;: [], \u0026#34;client_metadata\u0026#34;: { \u0026#34;features\u0026#34;: \u0026#34;0x00000000000001ff\u0026#34;, \u0026#34;entity_id\u0026#34;: [...], \u0026#34;hostname\u0026#34;: [...], \u0026#34;kernel_version\u0026#34;: \u0026#34;4.19.[...]\u0026#34;, \u0026#34;root\u0026#34;: [...] } Short note: there\u0026rsquo;s no documentation explaining those codes and its respective features, so I\u0026rsquo;m going to look at the source code and try to come up with a table. I\u0026rsquo;ll post it as soon as I manage to.\nSearching further I came across the command ceph daemon mon.hostname sessions:\n[root@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep jewel \u0026#34;MonSession(client.354037 v1:[...]]:0/3173650400 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.394132 v1:[...]:0/295867623 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.785407 v1:[...]:0/507474248 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.367883 v1:[...]:0/3274665177 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.812187 v1:[...]:0/733157529 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.1390811 v1:[...]:0/1470215921 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.1400452 v1:[...]:0/3851374868 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, Now I started to see some light at the end, since that command gave me the ip address and the version for each client connected. I knew that we didn\u0026rsquo;t have any clients using ceph-fuse, so it was safe to assume that everyone was using the kernel module to mount either RBD or cephfs. That cleared some confusion I had at the beginning between the ceph-common package version and the kernel version. It is not required to have that package installed in order to use the kernel module, so I then pointed my efforts at checking the clients\u0026rsquo; kernel versions.\nThe documentation also recommended some debugging at mon:\nThe ceph features command that reports the total number of clients and daemons and their features and releases. If the debugging level for Monitors is set to 10 (debug mon = 10), addresses and features of connecting and disconnecting clients are logged to log file on a local file system.\nSo I tried that and here\u0026rsquo;s what we get with debug mon = 10:\n2020-11-10 08:55:59.084 7f31c9015700 0 --1- [v2:[...]:3300/0,v1:[...]:6789/0] \u0026gt;\u0026gt; conn(0x561a94aed180 0x561a94ba5000 :6789 s=ACCE PTING pgs=0 cs=0 l=0).handle_client_banner accept peer addr is really - (socket is v1:[...]:40764/0) 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader) e1 ms_handle_accept con 0x561a94aed180 no session 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader) e1 _ms_dispatch new session 0x561a94adcd80 MonSession(unknown.0 - is open , featu res 0x27018fb86aa42ada (jewel)) features 0x27018fb86aa42ada 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 preprocess_query auth(proto 0 34 bytes epoch 0) from unknown.0 - 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 prep_auth() blob_size=34 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 _assign_global_id 34110 (max 44096) 2020-11-10 08:55:59.085 7f31c500d700 2 mon.ceph-1@0(leader) e1 send_reply 0x561a958ba780 0x561a95246fc0 auth_reply(proto 2 0 (0) Success) v1 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 preprocess_query auth(proto 2 32 bytes epoch 0) from unknown.0 - 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 prep_auth() blob_size=32 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader) e1 ms_handle_authentication session 0x561a94adcd80 con 0x561a94aed180 addr - MonSession(unknown.0 - is open , features 0x27018fb86aa42ada (jewel)) Without debug mon = 10, only the first line is logged.\nThat is very useful for detecting clients\u0026rsquo; features when they are connecting, but not so much when you can\u0026rsquo;t have the luxury to reset everyone\u0026rsquo;s connection. Besides, leaving that level of debugging enabled all the time would flag our node as an abuser at the log server in a matter of hours.\nWhat I knew already about our clients was that there was some elastic nodes using RBD, probably with CentOS 7 and consequently using kernel 3.x; Some other stuff also using RBD on either CentOS 7 or CentOS 8; and a kubernetes cluster using CoreOS and Flatcar with kernel 4.x. So after some checking, I compared the output from a client using kernel 4.19.x and another with 5.4.x:\n4.19.x:\n[root@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep [...] \u0026#34;MonSession(client.95408384 [...]:0/923650579 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.95446387 [...]:0/421761626 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(client.95427342 [...]:0/1106672011 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/2823489235 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/4141488860 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/662892667 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/940509445 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/1147570690 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026#34;, 5.4.x:\nroot@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep [...] \u0026#34;MonSession(unknown.0 [...]:0/2258512824 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/4186418195 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/232634731 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(client.95587947 [...]:0/3963151684 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/1164059037 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/1228306108 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/2811111809 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/4254695077 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(client.95587926 [...]:0/479729343 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/340770437 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, \u0026#34;MonSession(unknown.0 [...]:0/425381347 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026#34;, After that I could take some conclusion. We\u0026rsquo;ll probably keep seeing jewel features for a long time, since CentOS/RHEL 8 and most of the main distros uses kernel 4.x on their latest LTS versions. I expected that at kernel 3.x I\u0026rsquo;d get Jewel features and from 4.x on we would get Luminous, but apparently the latest features are only available on kernels 5.x and beyond. I\u0026rsquo;m not sure if there is any feature that would be marked as nautilus or any newer ceph version, so I still need to dig deeper on that subject.\nReferences:\nhttps://ceph.io/community/new-luminous-upgrade-complete/\nhttps://docs.ceph.com/en/latest/rados/operations/upmap/#enabling\nhttps://docs.ceph.com/en/latest/man/8/ceph/#osd\nhttps://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3.0/html/release_notes/major_updates\n","date":"16 Nov, 2020","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/ceph/cephalopod_huaf4bfc1960f31a189d113f153d6b773c_450678_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/cephalopod_huaf4bfc1960f31a189d113f153d6b773c_450678_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/cephalopod_huaf4bfc1960f31a189d113f153d6b773c_450678_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/cephalopod_huaf4bfc1960f31a189d113f153d6b773c_450678_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/ceph/cephalopod_huaf4bfc1960f31a189d113f153d6b773c_450678_1110x0_resize_box_3.png\" alt=\"\" width=\"2512\" height=\"410\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/ceph-features/","tags":["Ceph"],"title":"Ceph features and its relation to the client kernel version"},{"categories":["Ceph","Python"],"contents":"After some digging I found the list of possible features, its respective kernel version requirement and/or when it became available.\nThis can be found in src/include/ceph_features.h, and I got it by cloning the ceph repo at git://github.com/ceph/ceph .\nThat information might be useful when you are trying to determine if your clients might need a kernel upgrade and what kind of RBD or cephfs features you can enable on your server side without breaking compatibility. It\u0026rsquo;s always ideal to have every new feature available whenever possible, but that is not always the case when you have a medium to large deployment, multiple clients with different workloads and scenarios - in other words, a Real World situation.\nimport sys feature_list = [ \u0026#34;( 0, 1, UID)\u0026#34;, \u0026#34;( 1, 1, NOSRCADDR) // 2.6.35 req\u0026#34;, \u0026#34;( 2, 3, SERVER_NAUTILUS)\u0026#34;, \u0026#34;( 3, 1, FLOCK) // 2.6.36\u0026#34;, \u0026#34;( 4, 1, SUBSCRIBE2) // 4.6 req\u0026#34;, \u0026#34;( 5, 1, MONNAMES)\u0026#34;, \u0026#34;( 6, 1, RECONNECT_SEQ) // 3.10 req\u0026#34;, \u0026#34;( 7, 1, DIRLAYOUTHASH) // 2.6.38\u0026#34;, \u0026#34;( 8, 1, OBJECTLOCATOR)\u0026#34;, \u0026#34;( 9, 1, PGID64) // 3.9 req\u0026#34;, \u0026#34;(10, 1, INCSUBOSDMAP)\u0026#34;, \u0026#34;(11, 1, PGPOOL3) // 3.9 req\u0026#34;, \u0026#34;(12, 1, OSDREPLYMUX)\u0026#34;, \u0026#34;(13, 1, OSDENC) // 3.9 req\u0026#34;, \u0026#34;(14, 2, SERVER_KRAKEN)\u0026#34;, \u0026#34;(15, 1, MONENC)\u0026#34;, \u0026#34;(16, 3, SERVER_OCTOPUS) | (16, 3, OSD_REPOP_MLCOD)\u0026#34;, \u0026#34;(17, 3, OS_PERF_STAT_NS)\u0026#34;, \u0026#34;(18, 1, CRUSH_TUNABLES) // 3.6\u0026#34;, \u0026#34;(19, 2, OSD_PGLOG_HARDLIMIT)\u0026#34;, \u0026#34;(20, 3, SERVER_PACIFIC)\u0026#34;, \u0026#34;(21, 2, SERVER_LUMINOUS) // 4.13 | (21, 2, RESEND_ON_SPLIT) | (21, 2, RADOS_BACKOFF) | (21, 2, OSDMAP_PG_UPMAP) | (21, 2, CRUSH_CHOOSE_ARGS)\u0026#34;, \u0026#34;(22, 2, OSD_FIXED_COLLECTION_LIST)\u0026#34;, \u0026#34;(23, 1, MSG_AUTH) // 3.19 req (unless nocephx_require_signatures)\u0026#34;, \u0026#34;(24, 2, RECOVERY_RESERVATION_2)\u0026#34;, \u0026#34;(25, 1, CRUSH_TUNABLES2) // 3.9\u0026#34;, \u0026#34;(26, 1, CREATEPOOLID)\u0026#34;, \u0026#34;(27, 1, REPLY_CREATE_INODE) // 3.9\u0026#34;, \u0026#34;(28, 2, SERVER_MIMIC)\u0026#34;, \u0026#34;(29, 1, MDSENC) // 4.7\u0026#34;, \u0026#34;(30, 1, OSDHASHPSPOOL) // 3.9\u0026#34;, \u0026#34;DEPRECATED(31, 1, MON_SINGLE_PAXOS, NAUTILUS)\u0026#34;, \u0026#34;(32, 3, STRETCH_MODE)\u0026#34;, \u0026#34;RETIRED(33, 1, MON_SCRUB, JEWEL, LUMINOUS)\u0026#34;, \u0026#34;RETIRED(34, 1, OSD_PACKED_RECOVERY, JEWEL, LUMINOUS)\u0026#34;, \u0026#34;(35, 1, OSD_CACHEPOOL) // 3.14\u0026#34;, \u0026#34;(36, 1, CRUSH_V2) // 3.14\u0026#34;, \u0026#34;(37, 1, EXPORT_PEER) // 3.14\u0026#34;, \u0026#34;RETIRED(38, 1, OSD_ERASURE_CODES, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;(39, 1, OSDMAP_ENC) // 3.15\u0026#34;, \u0026#34;(40, 1, MDS_INLINE_DATA) // 3.19\u0026#34;, \u0026#34;(41, 1, CRUSH_TUNABLES3) // 3.15 | (41, 1, OSD_PRIMARY_AFFINITY)\u0026#34;, \u0026#34;(42, 1, MSGR_KEEPALIVE2) // 4.3 (for consistency)\u0026#34;, \u0026#34;(43, 1, OSD_POOLRESEND) // 4.13\u0026#34;, \u0026#34;RETIRED(44, 1, ERASURE_CODE_PLUGINS_V2, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(45, 1, OSD_SET_ALLOC_HINT, JEWEL, LUMINOUS)\u0026#34;, \u0026#34;(46, 1, OSD_FADVISE_FLAGS)\u0026#34;, \u0026#34;(47, 1, MDS_QUOTA) // 4.17\u0026#34;, \u0026#34;(48, 1, CRUSH_V4) // 4.1\u0026#34;, \u0026#34;RETIRED(49, 1, OSD_MIN_SIZE_RECOVERY, JEWEL, LUMINOUS)\u0026#34;, \u0026#34;RETIRED(50, 1, MON_METADATA, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(51, 1, OSD_BITWISE_HOBJ_SORT, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(52, 1, OSD_PROXY_WRITE_FEATURES, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(53, 1, ERASURE_CODE_PLUGINS_V3, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(54, 1, OSD_HITSET_GMT, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;RETIRED(55, 1, HAMMER_0_94_4, MIMIC, OCTOPUS)\u0026#34;, \u0026#34;(56, 1, NEW_OSDOP_ENCODING) // 4.13 (for pg_pool_t \u0026gt;= v25)\u0026#34;, \u0026#34;(57, 1, MON_STATEFUL_SUB) // 4.13 | (57, 1, SERVER_JEWEL)\u0026#34;, \u0026#34;(58, 1, CRUSH_TUNABLES5) // 4.5 | (58, 1, NEW_OSDOPREPLY_ENCODING) | (58, 1, FS_FILE_LAYOUT_V2)\u0026#34;, \u0026#34;(59, 1, FS_BTIME) | (59, 1, FS_CHANGE_ATTR) | (59, 1, MSG_ADDR2)\u0026#34;, \u0026#34;(60, 1, OSD_RECOVERY_DELETES) // *do not share this bit*\u0026#34;, \u0026#34;(61, 1, CEPHX_V2) // 4.19, *do not share this bit*\u0026#34;, \u0026#34;(62, 1, RESERVED) // do not use; used as a sentinel\u0026#34;, \u0026#34;DEPRECATED(63, 1, RESERVED_BROKEN, LUMINOUS) // client-facing\u0026#34; ] def settolist(featureset): \u0026#39;\u0026#39;\u0026#39;Converts a featureset hex to a feature list. Parameters: int:featureset Returns: list:featureset_list \u0026#39;\u0026#39;\u0026#39; featureset_hex = featureset featureset_list = [] featureset_bin = bin(int(featureset_hex, 16))[2:].zfill(64) count = 0 for bit in (featureset_bin): if bit == \u0026#39;1\u0026#39;: featureset_list.append(feature_list[count]) count += 1 return featureset_list output = settolist(str(sys.argv[1])) print(\u0026#34;\\n\u0026#34;.join(output)) A test run using an entry from my test cluster:\nMonSession(client.95280473 [IP]:0/110518927 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;\npython features.py 0x27018fb86aa42ada ( 2, 3, SERVER_NAUTILUS) ( 5, 1, MONNAMES) ( 6, 1, RECONNECT_SEQ) // 3.10 req ( 7, 1, DIRLAYOUTHASH) // 2.6.38 (15, 1, MONENC) (16, 3, SERVER_OCTOPUS) | (16, 3, OSD_REPOP_MLCOD) (20, 3, SERVER_PACIFIC) (21, 2, SERVER_LUMINOUS) // 4.13 | (21, 2, RESEND_ON_SPLIT) | (21, 2, RADOS_BACKOFF) | (21, 2, OSDMAP_PG_UPMAP) | (21, 2, CRUSH_CHOOSE_ARGS) (22, 2, OSD_FIXED_COLLECTION_LIST) (23, 1, MSG_AUTH) // 3.19 req (unless nocephx_require_signatures) (24, 2, RECOVERY_RESERVATION_2) (26, 1, CREATEPOOLID) (27, 1, REPLY_CREATE_INODE) // 3.9 (28, 2, SERVER_MIMIC) RETIRED(33, 1, MON_SCRUB, JEWEL, LUMINOUS) RETIRED(34, 1, OSD_PACKED_RECOVERY, JEWEL, LUMINOUS) (36, 1, CRUSH_V2) // 3.14 RETIRED(38, 1, OSD_ERASURE_CODES, MIMIC, OCTOPUS) (40, 1, MDS_INLINE_DATA) // 3.19 (42, 1, MSGR_KEEPALIVE2) // 4.3 (for consistency) RETIRED(45, 1, OSD_SET_ALLOC_HINT, JEWEL, LUMINOUS) RETIRED(50, 1, MON_METADATA, MIMIC, OCTOPUS) RETIRED(52, 1, OSD_PROXY_WRITE_FEATURES, MIMIC, OCTOPUS) RETIRED(54, 1, OSD_HITSET_GMT, MIMIC, OCTOPUS) (56, 1, NEW_OSDOP_ENCODING) // 4.13 (for pg_pool_t \u0026gt;= v25) (57, 1, MON_STATEFUL_SUB) // 4.13 | (57, 1, SERVER_JEWEL) (59, 1, FS_BTIME) | (59, 1, FS_CHANGE_ATTR) | (59, 1, MSG_ADDR2) (60, 1, OSD_RECOVERY_DELETES) // *do not share this bit* (62, 1, RESERVED) // do not use; used as a sentinel ","date":"16 Nov, 2020","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/ceph/ceph_banner_huc14bf7380ecee42ed6056d454b787f83_271072_545x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/ceph_banner_huc14bf7380ecee42ed6056d454b787f83_271072_600x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/ceph_banner_huc14bf7380ecee42ed6056d454b787f83_271072_700x0_resize_q95_h2_box_3.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/ceph_banner_huc14bf7380ecee42ed6056d454b787f83_271072_1110x0_resize_q95_h2_box_3.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/ceph/ceph_banner_huc14bf7380ecee42ed6056d454b787f83_271072_1110x0_resize_box_3.png\" alt=\"\" width=\"1943\" height=\"434\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/ceph-features-2/","tags":["Ceph","Python"],"title":"Obtaining a list of Ceph features from the hexadecimal value"},{"categories":["Python","Ceph"],"contents":"I needed to perform external monitoring on our S3 endpoints to better observe how the service was performing, thus getting a better representation of user experience. I use basically Python and Docker to accomplish the task.\nWe have two relatively large Ceph clusters (2.5PB, 600 OSDs) at the company that provides object storage using implementing the S3 API, so we can leverage the AWS SDK and use it to perform operations like creating buckets, putting objects and so on, and measuring the success and the time consumed to perform each operation. With those metrics we can then create graphics, panels and alerts using Grafana for instance. Using some kind of KISS methodology (keep it simple, stupid), running everything inside a docker container would give the flexibility to throw it anywhere and get the same results. The initial goal was to run two containers on AWS pointing to both sites (where the two main Ceph Clusters live) and two others on each site, one pointing to the other.\nFor the task I used Python as the programming language with the Prometheus Client for Python, found at Prometheus client_python. I divided the code in sections to talk about each part separatedly.\n1. Libraries I used: prometheus_client for metrics, boto3 for s3 connection and random.\nfrom prometheus_client import Histogram, start_http_server, Summary, Gauge, Counter import boto3 import random from io import BytesIO import os import time import logging 2. Variables Here we define our endpoint address, credentials, name of the bucket and objects, the interval between tests, the histogram buckets and how many objects we want to put simultaneously (for histogram metrics):\nCLUSTER_ENDPOINT = \u0026#39;\u0026#39; ACCESS_KEY = \u0026#39;\u0026#39; SECRET_ACCESS_KEY = \u0026#39;\u0026#39; BUCKET_NAME = \u0026#39;the_lord_of_buckets\u0026#39; OBJECT_NAME = \u0026#39;the_ruler\u0026#39; PORT=9102 TIME_TO_SLEEP = 120 LOG_LEVEL=logging.INFO BUCKETS=[0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95 ,1] OBJCOUNT=15 3. Metrics Here we define what metrics we want to generate, or collect: ALL_TESTSm S3_FAIL, CREATE, PUT, DELETE, GET_OBJECT, LIST_OBJECTS, REMOVE_BUCKET, DELETE_OBJECTS. For each operation there are three metrics defined: Summary, Gauge and Histogram. Labels were used for each operation.\nALL_TESTS_SUMMARY = Summary(\u0026#39;s3_request_duration_seconds\u0026#39;, \u0026#39;Time spent running the test\u0026#39;,[\u0026#39;task\u0026#39;]) ALL_TESTS_GAUGE = Gauge(\u0026#39;s3_request_last_duration_seconds\u0026#39;,\u0026#39;Time spent on the last test\u0026#39;,[\u0026#39;task\u0026#39;]) ALL_TESTS_HIST = Histogram(\u0026#39;s3_request_duration_seconds_histogram\u0026#39;,\u0026#39;Time spent on the last test\u0026#39;,[\u0026#39;task\u0026#39;],buckets=BUCKETS) S3_FAIL = Counter(\u0026#39;s3_requests_fails\u0026#39;, \u0026#39;Number of fails in this process\u0026#39;) CREATE_BUCKET_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;create\u0026#39;) CREATE_BUCKET_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;create\u0026#39;) CREATE_BUCKET_HIST = ALL_TESTS_HIST.labels(\u0026#39;create\u0026#39;) PUT_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;put\u0026#39;) PUT_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;put\u0026#39;) PUT_OBJECT_HIST = ALL_TESTS_HIST.labels(\u0026#39;put\u0026#39;) DELETE_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;delete\u0026#39;) DELETE_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;delete\u0026#39;) DELETE_OBJECT_HIST = ALL_TESTS_HIST.labels(\u0026#39;delete\u0026#39;) GET_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;get\u0026#39;) GET_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;get\u0026#39;) GET_OBJECT_HIST = ALL_TESTS_HIST.labels(\u0026#39;get\u0026#39;) LIST_OBJECTS_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;list_objects\u0026#39;) LIST_OBJECTS_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;list_objects\u0026#39;) LIST_OBJECTS_HIST = ALL_TESTS_HIST.labels(\u0026#39;list_objects\u0026#39;) LIST_BUCKETS_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;list_buckets\u0026#39;) LIST_BUCKETS_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;list_buckets\u0026#39;) LIST_BUCKETS_HIST = ALL_TESTS_HIST.labels(\u0026#39;list_buckets\u0026#39;) REMOVE_BUCKET_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;remove\u0026#39;) REMOVE_BUCKET_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;remove\u0026#39;) REMOVE_BUCKET_HIST = ALL_TESTS_HIST.labels(\u0026#39;remove\u0026#39;) DELETE_OBJECTS_SUMMARY = ALL_TESTS_SUMMARY.labels(\u0026#39;delete_many\u0026#39;) DELETE_OBJECTS_GAUGE = ALL_TESTS_GAUGE.labels(\u0026#39;delete_many\u0026#39;) DELETE_OBJECTS_HIST = ALL_TESTS_HIST.labels(\u0026#39;delete_many\u0026#39;) 4. Class S3Metrics This is where stuff gets done! There is an init function to set the parameters from the variables, start logging and create an a s3 resource from the boto3 library. Decorators were used to measure the time spend in each function and the run_all_tests funcion calls all the tests, which is the one we use as we\u0026rsquo;ll see next.\nclass S3Metrics: s3 = None bucket = None file_size = 512 data = None key_name = None data_out = None def __init__(self, bucket_name, key_name): cluster_endpoint = os.getenv(\u0026#39;CLUSTER_ENDPOINT\u0026#39;,default=CLUSTER_ENDPOINT) access_key = os.getenv(\u0026#39;ACCESS_KEY\u0026#39;,default=ACCESS_KEY) secret_access_key = os.getenv(\u0026#39;SECRET_ACCESS_KEY\u0026#39;,default=SECRET_ACCESS_KEY) log_level = os.getenv(\u0026#39;LOG_LEVEL\u0026#39;,default=LOG_LEVEL) self.bucket = bucket_name self.key_name = key_name self.generate_data() logging.basicConfig(level=int(log_level)) self.s3 = boto3.resource(\u0026#39;s3\u0026#39;, endpoint_url=cluster_endpoint, aws_access_key_id=access_key, aws_secret_access_key=secret_access_key) @CREATE_BUCKET_HIST.time() @CREATE_BUCKET_GAUGE.time() @CREATE_BUCKET_SUMMARY.time() def create_bucket(self): try: bucket = self.s3.Bucket(self.bucket) bucket.create() except: S3_FAIL.inc() @PUT_OBJECT_HIST.time() @PUT_OBJECT_GAUGE.time() @PUT_OBJECT_SUMMARY.time() def put_object(self,key_name=OBJECT_NAME): try: bucket = self.s3.Bucket(self.bucket) bucket.put_object( Key=key_name, Body=self.data) except: S3_FAIL.inc() def put_objects(self): for i in range(0,OBJCOUNT): self.put_object(self.key_name+str(i)) @DELETE_OBJECT_HIST.time() @DELETE_OBJECT_GAUGE.time() @DELETE_OBJECT_SUMMARY.time() def del_object(self): try: object = self.s3.Object(self.bucket,self.key_name) object.delete() except: S3_FAIL.inc() @DELETE_OBJECTS_HIST.time() @DELETE_OBJECTS_GAUGE.time() @DELETE_OBJECTS_SUMMARY.time() def del_objects(self): try: bucket = self.s3.Bucket(self.bucket) objects = bucket.objects.all() for object in objects: object.delete() except: S3_FAIL.inc() @GET_OBJECT_HIST.time() @GET_OBJECT_GAUGE.time() @GET_OBJECT_SUMMARY.time() def get_object(self): self.data_out = None try: bucket = self.s3.Bucket(self.bucket) data = BytesIO() bucket.download_fileobj(Fileobj=data, Key=self.key_name) self.data_out = data.getvalue().decode() except: S3_FAIL.inc() def compare_object(self): if self.data != self.data_out: S3_FAIL.inc() @LIST_BUCKETS_HIST.time() @LIST_BUCKETS_GAUGE.time() @LIST_BUCKETS_SUMMARY.time() def list_buckets(self): try: all_buckets = self.s3.buckets.all() for bucket in all_buckets: logging.debug(\u0026#34;%s %s\u0026#34;, bucket.creation_date, bucket.name) except: S3_FAIL.inc() @LIST_OBJECTS_HIST.time() @LIST_OBJECTS_GAUGE.time() @LIST_OBJECTS_SUMMARY.time() def list_objects(self): try: bucket = self.s3.Bucket(self.bucket) objects = bucket.objects.all() for object in objects: logging.debug(\u0026#34;%s\u0026#34;, object.key) except: S3_FAIL.inc() @REMOVE_BUCKET_HIST.time() @REMOVE_BUCKET_GAUGE.time() @REMOVE_BUCKET_SUMMARY.time() def remove_bucket(self): try: bucket = self.s3.Bucket(self.bucket) bucket.delete() except: S3_FAIL.inc() def run_all_tests(self): self.create_bucket() self.put_object() self.list_buckets() self.list_objects() self.get_object() self.compare_object() self.del_object() self.put_objects() self.del_objects() self.remove_bucket() def generate_data(self): MAX_LIMIT = 126 data = \u0026#39;\u0026#39; for _ in range(self.file_size): random_integer = random.randint(32, MAX_LIMIT) data += (chr(random_integer)) self.data = data 5. Main Here we start the prometheus exporter and call the method run_all_tests() from class S3Metrics inside an infinite while loop, so the script will run continuously until manually stopped.\ndef main(): start_http_server(PORT) s3metrics = S3Metrics(BUCKET_NAME, OBJECT_NAME) while True: s3metrics.run_all_tests() time.sleep(TIME_TO_SLEEP) if __name__ == \u0026#39;__main__\u0026#39;: main() So now that we have our s3metrics.py, we have to build a container to run it. I created a requirements.txt file to make it easier to adjust the package versions in the future, and used in the Dockerfile the python:3 base image since it already have most of what I need.\n6. Requirements boto3==1.17.54 botocore==1.20.54 jmespath==0.10.0 prometheus-client==0.10.1 python-dateutil==2.8.1 s3transfer==0.4.0 six==1.15.0 urllib3==1.26.4 7. Dockerfile Using the python:3 base image and putting our .py file inside, we can run it inside a container:\nFROM python:3 WORKDIR /app COPY requirements.txt . COPY s3-metrics.py . RUN python3 -m venv .venv; bash -c \u0026#39;source .venv/bin/activate\u0026#39;; pip install -r requirements.txt CMD [ \u0026#34;python\u0026#34;, \u0026#34;./s3-metrics.py\u0026#34; ] 8. Build and Run Now we can run the container inside an EC2 instance or anywhere capable of running a docker container (you don\u0026rsquo;t say, lol). I\u0026rsquo;ll later post my terraform script used to deploy it on AWS with more details. Now for the Grafana part:\n9. Grafana Here\u0026rsquo;s how I configured a Bar gauge graph to see the PUT Histogram:\nWe can see that the majority of the operations take less than 0.5 seconds to finish, so if we start seeing an increase in the other histogram buckets, it means that we probably have a problem.\nI also created a panel to watch for errors and one to see all the operations combined:\n10. Conclusion That kind of monitoring can give a good observability to a service like Ceph S3 Gateway, but this could easily be applied to any other service with some changes. Before that, we used UpTimeRobot to achieve a similar goal, but from my perspective you can have much more control and flexibility with a container like that performing connectivity tests, and in this case we go much deeper by mimicking user operations on our tests.\n","date":"20 Sep, 2020","image":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\u003cpicture\u003e\r\n  \u003csource srcset=\"/images/ceph/noc_hu275cd9a2fc4525df33f8dba365d9bdc2_97683_545x0_resize_q95_h2_box.webp\" media=\"(max-width: 575px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/noc_hu275cd9a2fc4525df33f8dba365d9bdc2_97683_600x0_resize_q95_h2_box.webp\" media=\"(max-width: 767px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/noc_hu275cd9a2fc4525df33f8dba365d9bdc2_97683_700x0_resize_q95_h2_box.webp\" media=\"(max-width: 991px)\"\u003e\r\n  \u003csource srcset=\"/images/ceph/noc_hu275cd9a2fc4525df33f8dba365d9bdc2_97683_1110x0_resize_q95_h2_box.webp\"\u003e\r\n  \u003cimg loading=\"lazy\" decoding=\"async\" class=\"w-100 img-fluid rounded\" src=\"/images/ceph/noc_hu275cd9a2fc4525df33f8dba365d9bdc2_97683_1110x0_resize_q95_box.jpeg\" alt=\"\" width=\"640\" height=\"339\"\u003e\r\n\u003c/picture\u003e\r\n \r\n \r\n \r\n\r\n","permalink":"https://www.fabiolopes.page/blog/python-s3-monitoring/","tags":["Python","Ceph"],"title":"Using Python and Docker to monitor Ceph S3 from the internet"}]