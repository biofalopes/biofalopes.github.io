[{
    "title": "Getting started with Jenkins Declarative Pipelines",
    "date": "",
    "description": "My approach on running declarative pipelines on Jenkins to build, test and deploy a sample springboot application, using Docker and some useful integrations like Git and Slack.",
    "body": "After hours and hours of training, videos, documentation, and tutorials, it was time to put into practice the concepts I learned about Jenkins, one of the most used automation tools for CI/CD.\n It is known that there are many other more modern solutions today, like GitHub Actions or Gitlab CI, for instance, which is a common approach since you can leverage the SCM directly to automate your tasks instead of depending on an external tool. However, Jenkins is still widely adopted and you can perform many automation tasks without much effort using it. It\u0026rsquo;s all going to depend on the situation, the current project, and the team. Traditionally, Jenkins jobs were created using the Jenkins UI and were called FreeStyle jobs. In Jenkins 2.0, a new way was introduced using a technique called pipeline as code, where jobs are created using a script file containing the steps to be executed. That scripted file is called Jenkinsfile. That\u0026rsquo;s just a glimpse on Jenkins pipelines, there\u0026rsquo;s much more depth to it and you can always dive deeper using the official documentation at https://www.jenkins.io/doc/book/pipeline/.\nBefore we start to get into the real stuff, let me just mention a reference that inevitably came into my mind when I first read the name \u0026ldquo;Jenkins\u0026rdquo;, which I\u0026rsquo;m sure that most gamers from early 2000s are gonna remember:\nIf you\u0026rsquo;d like to watch the video, here\u0026rsquo;s the YouTube link to it: Leeroy Jenkins HD 1080p\nAlright, let’s start with the basics.\n1. What\u0026rsquo;s a CI/CD Pipeline? A CI/CD pipeline is usually described as the processes and stages of automation on software delivery. There\u0026rsquo;s usually a specific tool used to achieve this, that orchestrates with SCM and the other tools. It can be used to build code, run unit tests, smoke tests, call external tools like SonarQube, Anchore, Trivy, build Docker images and deploy to environments like Kubernetes or wherever the application is delivered. By performing standardized operations and tests, a CI/CD pipeline can help reduce manual errors, provide feedback to developers, and allow fast iterations.\n2. What\u0026rsquo;s a Jenkinsfile? A Jenkinsfile is just a text file, usually checked in along with the project\u0026rsquo;s source code in an SCM. Ideally, each application will have its Jenkinsfile. A Jenkinsfile can be written in two ways - \u0026ldquo;scripted pipeline syntax\u0026rdquo; or \u0026ldquo;declarative pipeline syntax\u0026rdquo;.\n3. What\u0026rsquo;s a Jenkins Scripted Pipeline? Scripted pipelines run on the Jenkins master with the help of a lightweight executor. It uses very few resources to translate the pipeline into atomic commands. Both declarative and scripted syntax are different from each other and are defined differently. Scripted pipelines are written in Groovy.\n It requires knowledge of the Groovy language; The Jenkinsfile starts with the word \u0026lsquo;node\u0026rsquo;; Advanced cabapilities; Groovy engine; Can contain standard programming constructs like if-else block, try-catch block, etc.  An example of a Scripted Pipeline:\nnode { stage('Build') { if (env.BRANCH_NAME == 'master') { echo \u0026quot;I'm on master brand, building application\u0026quot; bat \u0026quot;msbuild ${C:\\\\Jenkins\\\\my_project\\\\workspace\\\\test\\\\my_project.sln}\u0026quot; } else { echo 'Towards the center of the Cloud Computer.' } } stage('Selenium') { echo 'Running Selenium Smoke Tests' dir(bachelol) { //changes the path to “bachelol” bat \u0026quot;mvn clean test -Dsuite=SMOKE_TEST -Denvironment=QA\u0026quot; } } } 4. What\u0026rsquo;s a Jenkins Declarative Pipeline? The Declarative Pipeline is relatively new and provides a simplified, opinionated syntax on top of the Pipeline subsystems. Its syntax offers an easy way to create pipelines. It contains a predefined hierarchy and gives you the ability to control all aspects of a pipeline execution in a simple, straightforward manner.\n The latest addition in Jenkins pipeline job creation technique; Needs to use the predefined constructs to create pipelines; Hence, it is not flexible as a scripted pipeline; The Jenkinsfile starts with the word \u0026lsquo;pipeline\u0026rsquo;, and there is a predefined structure; Usually, the preferred way to start, as they offer a rich set of features, come with less learning curve \u0026amp; no prerequisite to learn a programming language like Groovy just for the sake of writing pipeline code; We can also validate the syntax of the Declarative pipeline code before running the job. It helps to avoid a lot of runtime issues with the build script.  5. Why run Jenkins on Docker? Despite the inherent advantages (and caveats) of running anything in Docker, the main reasons I could think of are:\n Ability to keep the server configuration under version control; Run multiple copies of the server anywhere; Integrate with Kubernetes and other orchestration platforms; Jenkins' official Docker image is widely adopted and maintained; Simple implementation = simple administration.  The main (Drawback|Caveat|Problem|Issue|Flaw) is when you want to use agents with docker too. To do that you\u0026rsquo;re gonna need some implementation of Docker in Docker. There are a few ways to do that, but the two easiest ways are connecting the Jenkins container directly to the host\u0026rsquo;s Docker socket using -v /var/run/docker.sock:/var/run/docker.sock and changing the permissions, or enabling TCP on the Docker server-side.\nIf you can run your Jenkins server on Kubernetes, you can enable it to launch new agent pods inside the cluster which gives you even more flexibility. I\u0026rsquo;ll try that in the future since I\u0026rsquo;m out of credits on AWS and my dev cluster is behind a proxy, which is a real pain to deal with.\n6. Building a Jenkins Image The Dockerfile below creates an image based on the latest Jenkins LTS from Dockerhub, and preinstalls the plugins needed:\nFROM jenkins/jenkins:lts # Skip setup wizard ENV JAVA_OPTS=\u0026quot;-Djenkins.install.runSetupWizard=false\u0026quot; # Set docker group ARG DOCKER_GID=998 # Get plugins RUN /usr/local/bin/install-plugins.sh \\ workflow-multibranch:latest \\ pipeline-model-definition:latest \\ pipeline-stage-view:latest \\ git:latest \\ credentials:latest \\ slack:latest # Install Docker \u0026amp; docker-compose USER root RUN apt-get update \u0026amp;\u0026amp; \\ apt-get -y install apt-transport-https \\ ca-certificates \\ curl \\ gnupg2 \\ software-properties-common \u0026amp;\u0026amp; \\ curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;)/gpg \u0026gt; /tmp/dkey; apt-key add /tmp/dkey \u0026amp;\u0026amp; \\ add-apt-repository \\ \u0026quot;deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo \u0026quot;$ID\u0026quot;) \\ $(lsb_release -cs) \\ stable\u0026quot; \u0026amp;\u0026amp; \\ apt-get update \u0026amp;\u0026amp; \\ groupadd -g ${DOCKER_GID} docker \u0026amp;\u0026amp; \\ apt-get -y install docker-ce docker-ce-cli containerd.io RUN curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose \u0026amp;\u0026amp; chmod +x /usr/local/bin/docker-compose RUN usermod -aG docker jenkins USER jenkins I referred to the official documentation:\nhttps://docs.docker.com/compose/install/\nhttps://docs.docker.com/engine/install/ubuntu/\nTo spin up a Jenkins instance on my local machine, I used a docker-compose.yml file and put everything on the same folder. The jenkins service points to a dockerfile on the same path, and the container will be accessible on localhost:9080. I mapped the docker.sock to the container and matched the docker GID with the host, and also created and mapped a jenkins_home folder locally to persist data:\nversion: \u0026quot;3.8\u0026quot; services: jenkins: build: context: . dockerfile: Dockerfile ports: - \u0026quot;127.0.0.1:9080:8080\u0026quot; volumes: - ./jenkins_home:/var/jenkins_home - /var/run/docker.sock:/var/run/docker.sock restart: unless-stopped Now that we have a running Jenkins instance with the plugins installed and with access to the host\u0026rsquo;s dockerd, let\u0026rsquo;s start using it to automate a build.\n7. The Spring PetClinic Sample Application To test everything, I downloaded the \u0026ldquo;Spring PetClinic Sample Application\u0026rdquo; (can be found here: https://github.com/spring-projects/spring-petclinic), which is written in Java using Spring Boot and built with Maven. If you clone the official repository, you\u0026rsquo;ll get a Maven Wrapper bundled together that can be used to build the application. So before trying to automate anything with Jenkins, let\u0026rsquo;s first build, test, and run the application manually:\n# Clone the repo git clone https://github.com/spring-projects/spring-petclinic.git # Enter the directory cd spring-petclinic #Build the application ./mvnw package #Run the application java -jar target/*.jar The first time I ran the mvnw package command, it took a while to finish since it downloaded all the dependencies. After that, subsequent runs took around a minute to finish. That\u0026rsquo;s pretty much all we have to do to test if it builds and runs properly, I just followed the instructions from the repository without changing anything. If it runs, we know that the code should work inside the pipeline too. The changes will come when we use the Maven from Jenkins and try to run JUnit tests.\nOutput from the build:\n[INFO] Building jar: ./github/spring-petclinic/target/spring-petclinic-2.5.0-SNAPSHOT.jar [INFO] [INFO] --- spring-boot-maven-plugin:2.5.4:repackage (repackage) @ spring-petclinic --- [INFO] Replacing main artifact with repackaged archive [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 01:15 min [INFO] Finished at: 2021-10-19T11:35:43-03:00 [INFO] ------------------------------------------------------------------------ Output from java -jar target/*.jar:\njava -jar target/*.jar |\\ _,,,--,,_ /,`.-'`' ._ \\-;;,_ _______ __|,4- ) )_ .;.(__`'-'__ ___ __ _ ___ _______ | | '---''(_/._)-'(_\\_) | | | | | | | | | | _ | ___|_ _| | | | | |_| | | | __ _ _ | |_| | |___ | | | | | | | | | | \\ \\ \\ \\ | ___| ___| | | | _| |___| | _ | | _| \\ \\ \\ \\ | | | |___ | | | |_| | | | | | | |_ ) ) ) ) |___| |_______| |___| |_______|_______|___|_| |__|___|_______| / / / / ==================================================================/_/_/_/ :: Built with Spring Boot :: 2.5.4 2021-10-19 11:38:31.201 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : Starting PetClinicApplication v2.5.0-SNAPSHOT using Java 11.0.11 on my-machine with PID 39806 (./github/spring-petclinic/target/spring-petclinic-2.5.0-SNAPSHOT.jar started by biofa in ./github/spring-petclinic) 2021-10-19 11:38:31.204 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : No active profile set, falling back to default profiles: default 2021-10-19 11:38:32.230 INFO 39806 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode. 2021-10-19 11:38:32.288 INFO 39806 --- [ main] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 50 ms. Found 4 JPA repository interfaces. 2021-10-19 11:38:33.020 INFO 39806 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2021-10-19 11:38:33.036 INFO 39806 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2021-10-19 11:38:33.037 INFO 39806 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.52] 2021-10-19 11:38:33.128 INFO 39806 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2021-10-19 11:38:33.128 INFO 39806 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 1865 ms 2021-10-19 11:38:33.377 INFO 39806 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting... 2021-10-19 11:38:33.590 INFO 39806 --- [ main] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 2021-10-19 11:38:33.919 INFO 39806 --- [ main] org.ehcache.core.EhcacheManager : Cache 'vets' created in EhcacheManager. 2021-10-19 11:38:33.934 INFO 39806 --- [ main] org.ehcache.jsr107.Eh107CacheManager : Registering Ehcache MBean javax.cache:type=CacheStatistics,CacheManager=urn.X-ehcache.jsr107-default-config,Cache=vets 2021-10-19 11:38:33.941 INFO 39806 --- [ main] org.ehcache.jsr107.Eh107CacheManager : Registering Ehcache MBean javax.cache:type=CacheStatistics,CacheManager=urn.X-ehcache.jsr107-default-config,Cache=vets 2021-10-19 11:38:34.021 INFO 39806 --- [ main] o.hibernate.jpa.internal.util.LogHelper : HHH000204: Processing PersistenceUnitInfo [name: default] 2021-10-19 11:38:34.081 INFO 39806 --- [ main] org.hibernate.Version : HHH000412: Hibernate ORM core version 5.4.32.Final 2021-10-19 11:38:34.204 INFO 39806 --- [ main] o.hibernate.annotations.common.Version : HCANN000001: Hibernate Commons Annotations {5.1.2.Final} 2021-10-19 11:38:34.332 INFO 39806 --- [ main] org.hibernate.dialect.Dialect : HHH000400: Using dialect: org.hibernate.dialect.H2Dialect 2021-10-19 11:38:34.955 INFO 39806 --- [ main] o.h.e.t.j.p.i.JtaPlatformInitiator : HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform] 2021-10-19 11:38:34.962 INFO 39806 --- [ main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default' 2021-10-19 11:38:36.371 INFO 39806 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 13 endpoint(s) beneath base path '/actuator' 2021-10-19 11:38:36.441 INFO 39806 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path '' 2021-10-19 11:38:36.453 INFO 39806 --- [ main] o.s.s.petclinic.PetClinicApplication : Started PetClinicApplication in 5.753 seconds (JVM running for 6.136) 8. Using Maven on Jenkins There are a couple of different ways to use Maven with Jenkins. There is a Maven plugin for Jenkins, and we can install Maven on the Jenkins server and set it up at \u0026ldquo;Global Tool Configuration\u0026rdquo;. Since we want to use docker containers as executors or agents, that would not be an option. We could also use the wrapper that comes with the project, but a better and simple route would be to use the official Maven docker image, and that way we can have an agent always updated and completely separate Maven from our Jenkins server. This approach makes it easier to manage the environment and also allows for a single Jenkins server to work with different types of builds with a single installation, without having to embed every tool used on the server.\nI built a pipeline with six stages: Init, Build, Test and Build Image, Publish Image and Prod. Some of the main aspects are:\n It mounts the m2 directory, which is used by maven, locally to have cache between builds, and also settings.xml to enable setting custom parameters to Maven; Maven 3.8.3 with AdoptOpenJDK 11 Docker Image is used as an agent only for the stages that require it; Dockerhub credentials were added previously on Jenkins.  pipeline { agent none environment { DEPLOY_TO = 'staging' imageName = \u0026quot;fabioctba/spring-petclinic\u0026quot; registryCredential = 'dockerhub' dockerImage = '' } stages { stage('Init'){ agent any steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Starting\u0026quot; sh 'git --version' echo \u0026quot;Deploying to ${DEPLOY_TO}\u0026quot; } } stage('Build') { agent { docker { image 'maven:3.8.3-adoptopenjdk-11' args '-v m2:/root/.m2 -v settings.xml:/root/.m2/settings.xml' } } steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Building\u0026quot; sh 'mvn -B -DskipTests clean package' } } stage('Test') { agent { docker { image 'maven:3.8.3-adoptopenjdk-11' args '-v m2:/root/.m2 -v settings.xml:/root/.m2/settings.xml' } } steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Running Tests\u0026quot; sh 'mvn test' } post { always { archiveArtifacts artifacts: 'target/*.jar' junit 'target/surefire-reports/*.xml' } } } stage('Build Image') { agent any steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Building Image\u0026quot; script { dockerImage = docker.build imageName } } } stage('Publish Image') { agent any steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Building Image\u0026quot; script { docker.withRegistry( '', registryCredential ) { dockerImage.push(\u0026quot;$BUILD_NUMBER\u0026quot;) dockerImage.push('latest') } } } } stage('Prod') { agent any when { allOf { branch 'master'; environment name: 'DEPLOY_TO', value: 'production' } } steps { slackSend channel: '#jenkins', message: \u0026quot;${env.BUILD_ID} on ${env.JENKINS_URL} - Running Production stage\u0026quot; echo 'Some extra step when on production release' } } } } I used the Slack Plugin to integrate with Jenkins, following this documentation. I used only two variables just to illustrate what can be sent as a message, but a full list of environment variables can be found directly on Jenkins accessing http://localhost:9080/env-vars.html. Of course, you have to change localhost:9080 to your server\u0026rsquo;s address.\nThe Init stage prints the versions of the main tools used, useful to debug problems;\nThe Build stage runs mvn package, which creates the jar files;\nThe Test stage run the Unit tests. Spring Pet Clinic has a total of 40 tests configured, and that would depend on how the dev team created their tests;\nThe Build Image stage creates a Docker image and publishes it to the Dockerhub;\nThe Publish Image stage publishes the image to Dockerhub;\nThe Prod stage doesn\u0026rsquo;t do anything, it\u0026rsquo;s just an example of a conditional step that can be executed depending on a variable.\nNotice that there isn\u0026rsquo;t a stage to clone the repository at the beginning; this stage is implicit when you configure Jenkins to pull the pipeline from SCM, which is the plan here.\nHere\u0026rsquo;s the Dockerfile I used to publish the application:\nFROM openjdk:11-jre-slim RUN groupadd --gid 1000 java \\ \u0026amp;\u0026amp; useradd --uid 1000 --gid java --shell /bin/bash --create-home java USER java VOLUME /tmp WORKDIR /app COPY --chown=java:java ./target/*.jar /app/ CMD [\u0026quot;java\u0026quot;,\u0026quot;-Djava.security.egd=file:/dev/./urandom\u0026quot;,\u0026quot;-jar\u0026quot;,\u0026quot;/app/*.jar\u0026quot;] That is a working pipeline, and I wanted to include most of the concepts I saw in the courses. There are other features I\u0026rsquo;d like to try in the future like shared libraries and multi-branch pipelines, but this might go into a different post.\n9. Courses I either took or recommend: a. Jenkins, From Zero To Hero: Become a DevOps Jenkins Master Udemy, by Ricardo Andre Gonzalez Gomez, 2018\nhttps://www.udemy.com/course/jenkins-from-zero-to-hero/\nI found this course a little outdated and looks like the author stopped maintaining it in 2020. However, if you never used Jenkins before and want a cheap course to start, I strongly recommend this one. It\u0026rsquo;s also well recommended in many other sources and has a high rate at Udemy. Ricardo focuses a lot on Docker, so it might be better targeted to someone without any experience in Docker also.\nb. Continuous Integration with Jenkins Pluralsight, by different authors, updated constantly\nhttps://app.pluralsight.com/paths/skill/continuous-integration-with-jenkins\nPluralsight has a total of 8 courses, each with around 2 hours of duration, which makes the Jenkins Skill Path. I took four of them during the last Pluralsight Free Week and like other courses I took from Pluralsight, the quality was much superior. Unfortunately, Pluralsight costs a great amount of money, so I usually stay on the lookout for their periodically free weeks and free weekends. The good thing is that you can download the course material and most of the authors have GitHub repositories where you can check the code later.\n10. References https://stackoverflow.com/questions/44440164/what-are-the-advantages-of-running-jenkins-in-a-docker-container\nhttps://www.cinqict.nl/blog/building-a-jenkins-development-docker-image\nhttps://tomgregory.com/building-a-spring-boot-application-in-jenkins/\nIf you read it to the end, I\u0026rsquo;d like to hear your comments, and hope this helped you in some way. Cheers.\n.\n.\n.\n.\nAnd sorry for the long post... here's a potato. Source: @truth.potato\n",
    "ref": "/blog/jenkins-1/"
  },{
    "title": "Using Python and Docker to monitor Ceph S3 from outside",
    "date": "",
    "description": "I created a simple application that runs on a Docker container to test and generate metrics on a custom S3 service, in my case being Ceph.",
    "body": "I needed to perform external monitoring on our S3 endpoints to better observe how the service was performing, thus getting a better representation of user experience. I use basically Python and Docker to accomplish the task.\n We have two relatively large Ceph clusters (2.5PB, 600 OSDs) at the company that provides object storage using implementing the S3 API, so we can leverage the AWS SDK and use it to perform operations like creating buckets, putting objects and so on, and measuring the success and the time consumed to perform each operation. With those metrics we can then create graphics, panels and alerts using Grafana for instance. Using some kind of KISS methodology (keep it simple, stupid), running everything inside a docker container would give the flexibility to throw it anywhere and get the same results. The initial goal was to run two containers on AWS pointing to both sites (where the two main Ceph Clusters live) and two others on each site, one pointing to the other.\nFor the task I used Python as the programming language with the Prometheus Client for Python, found at https://github.com/prometheus/client_python. I divided the code in sections to talk about each part.\n Libraries  prometheus_client for metrics, boto3 for s3 connection, random\nfrom prometheus_client import Histogram, start_http_server, Summary, Gauge, Counter import boto3 import random from io import BytesIO import os import time import logging Variables  Here we define our endpoint address, credentials, name of the bucket and objects, the interval between tests, the histogram buckets and how many objects we want to put simultaneously (for histogram metrics):\nCLUSTER_ENDPOINT = '' ACCESS_KEY = '' SECRET_ACCESS_KEY = '' BUCKET_NAME = 'the_lord_of_buckets' OBJECT_NAME = 'the_ruler' PORT=9102 TIME_TO_SLEEP = 120 LOG_LEVEL=logging.INFO BUCKETS=[0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95 ,1] OBJCOUNT=15 Metrics  Here we define what metrics we want to generate, or collect: ALL_TESTSm S3_FAIL, CREATE, PUT, DELETE, GET_OBJECT, LIST_OBJECTS, REMOVE_BUCKET, DELETE_OBJECTS. For each operation there are three metrics defined: Summary, Gauge and Histogram. Labels were used for each operation.\nALL_TESTS_SUMMARY = Summary('s3_request_duration_seconds', 'Time spent running the test',['task']) ALL_TESTS_GAUGE = Gauge('s3_request_last_duration_seconds','Time spent on the last test',['task']) ALL_TESTS_HIST = Histogram('s3_request_duration_seconds_histogram','Time spent on the last test',['task'],buckets=BUCKETS) S3_FAIL = Counter('s3_requests_fails', 'Number of fails in this process') CREATE_BUCKET_SUMMARY = ALL_TESTS_SUMMARY.labels('create') CREATE_BUCKET_GAUGE = ALL_TESTS_GAUGE.labels('create') CREATE_BUCKET_HIST = ALL_TESTS_HIST.labels('create') PUT_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels('put') PUT_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels('put') PUT_OBJECT_HIST = ALL_TESTS_HIST.labels('put') DELETE_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels('delete') DELETE_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels('delete') DELETE_OBJECT_HIST = ALL_TESTS_HIST.labels('delete') GET_OBJECT_SUMMARY = ALL_TESTS_SUMMARY.labels('get') GET_OBJECT_GAUGE = ALL_TESTS_GAUGE.labels('get') GET_OBJECT_HIST = ALL_TESTS_HIST.labels('get') LIST_OBJECTS_SUMMARY = ALL_TESTS_SUMMARY.labels('list_objects') LIST_OBJECTS_GAUGE = ALL_TESTS_GAUGE.labels('list_objects') LIST_OBJECTS_HIST = ALL_TESTS_HIST.labels('list_objects') LIST_BUCKETS_SUMMARY = ALL_TESTS_SUMMARY.labels('list_buckets') LIST_BUCKETS_GAUGE = ALL_TESTS_GAUGE.labels('list_buckets') LIST_BUCKETS_HIST = ALL_TESTS_HIST.labels('list_buckets') REMOVE_BUCKET_SUMMARY = ALL_TESTS_SUMMARY.labels('remove') REMOVE_BUCKET_GAUGE = ALL_TESTS_GAUGE.labels('remove') REMOVE_BUCKET_HIST = ALL_TESTS_HIST.labels('remove') DELETE_OBJECTS_SUMMARY = ALL_TESTS_SUMMARY.labels('delete_many') DELETE_OBJECTS_GAUGE = ALL_TESTS_GAUGE.labels('delete_many') DELETE_OBJECTS_HIST = ALL_TESTS_HIST.labels('delete_many') Class S3Metrics  This is where stuff gets done! There is an init function to set the parameters from the variables, start logging and create an a s3 resource from the boto3 library. Decorators were used to measure the time spend in each function and the run_all_tests funcion calls all the tests, which is the one we use as we\u0026rsquo;ll see next.\nclass S3Metrics: s3 = None bucket = None file_size = 512 data = None key_name = None data_out = None def __init__(self, bucket_name, key_name): cluster_endpoint = os.getenv('CLUSTER_ENDPOINT',default=CLUSTER_ENDPOINT) access_key = os.getenv('ACCESS_KEY',default=ACCESS_KEY) secret_access_key = os.getenv('SECRET_ACCESS_KEY',default=SECRET_ACCESS_KEY) log_level = os.getenv('LOG_LEVEL',default=LOG_LEVEL) self.bucket = bucket_name self.key_name = key_name self.generate_data() logging.basicConfig(level=int(log_level)) self.s3 = boto3.resource('s3', endpoint_url=cluster_endpoint, aws_access_key_id=access_key, aws_secret_access_key=secret_access_key) @CREATE_BUCKET_HIST.time() @CREATE_BUCKET_GAUGE.time() @CREATE_BUCKET_SUMMARY.time() def create_bucket(self): try: bucket = self.s3.Bucket(self.bucket) bucket.create() except: S3_FAIL.inc() @PUT_OBJECT_HIST.time() @PUT_OBJECT_GAUGE.time() @PUT_OBJECT_SUMMARY.time() def put_object(self,key_name=OBJECT_NAME): try: bucket = self.s3.Bucket(self.bucket) bucket.put_object( Key=key_name, Body=self.data) except: S3_FAIL.inc() def put_objects(self): for i in range(0,OBJCOUNT): self.put_object(self.key_name+str(i)) @DELETE_OBJECT_HIST.time() @DELETE_OBJECT_GAUGE.time() @DELETE_OBJECT_SUMMARY.time() def del_object(self): try: object = self.s3.Object(self.bucket,self.key_name) object.delete() except: S3_FAIL.inc() @DELETE_OBJECTS_HIST.time() @DELETE_OBJECTS_GAUGE.time() @DELETE_OBJECTS_SUMMARY.time() def del_objects(self): try: bucket = self.s3.Bucket(self.bucket) objects = bucket.objects.all() for object in objects: object.delete() except: S3_FAIL.inc() @GET_OBJECT_HIST.time() @GET_OBJECT_GAUGE.time() @GET_OBJECT_SUMMARY.time() def get_object(self): self.data_out = None try: bucket = self.s3.Bucket(self.bucket) data = BytesIO() bucket.download_fileobj(Fileobj=data, Key=self.key_name) self.data_out = data.getvalue().decode() except: S3_FAIL.inc() def compare_object(self): if self.data != self.data_out: S3_FAIL.inc() @LIST_BUCKETS_HIST.time() @LIST_BUCKETS_GAUGE.time() @LIST_BUCKETS_SUMMARY.time() def list_buckets(self): try: all_buckets = self.s3.buckets.all() for bucket in all_buckets: logging.debug(\u0026quot;%s %s\u0026quot;, bucket.creation_date, bucket.name) except: S3_FAIL.inc() @LIST_OBJECTS_HIST.time() @LIST_OBJECTS_GAUGE.time() @LIST_OBJECTS_SUMMARY.time() def list_objects(self): try: bucket = self.s3.Bucket(self.bucket) objects = bucket.objects.all() for object in objects: logging.debug(\u0026quot;%s\u0026quot;, object.key) except: S3_FAIL.inc() @REMOVE_BUCKET_HIST.time() @REMOVE_BUCKET_GAUGE.time() @REMOVE_BUCKET_SUMMARY.time() def remove_bucket(self): try: bucket = self.s3.Bucket(self.bucket) bucket.delete() except: S3_FAIL.inc() def run_all_tests(self): self.create_bucket() self.put_object() self.list_buckets() self.list_objects() self.get_object() self.compare_object() self.del_object() self.put_objects() self.del_objects() self.remove_bucket() def generate_data(self): MAX_LIMIT = 126 data = '' for _ in range(self.file_size): random_integer = random.randint(32, MAX_LIMIT) data += (chr(random_integer)) self.data = data Main  Here we start the prometheus exporter and call the method run_all_tests() from class S3Metrics inside an infinite while loop, so the script will run continuously until manually stopped.\ndef main(): start_http_server(PORT) s3metrics = S3Metrics(BUCKET_NAME, OBJECT_NAME) while True: s3metrics.run_all_tests() time.sleep(TIME_TO_SLEEP) if __name__ == '__main__': main() So now that we have our s3metrics.py, we have to build a container to run it. I created a requirements.txt file to make it easier to adjust the package versions in the future, and used in the Dockerfile the python:3 base image since it already have most of what I need.\nRequirements  boto3==1.17.54 botocore==1.20.54 jmespath==0.10.0 prometheus-client==0.10.1 python-dateutil==2.8.1 s3transfer==0.4.0 six==1.15.0 urllib3==1.26.4 Dockerfile  FROM python:3 WORKDIR /app COPY requirements.txt . COPY s3-metrics.py . RUN python3 -m venv .venv; bash -c 'source .venv/bin/activate'; pip install -r requirements.txt CMD [ \u0026quot;python\u0026quot;, \u0026quot;./s3-metrics.py\u0026quot; ] Build and Run  Now we can run the container inside an EC2 instance or anywhere capable of running a docker container (you don\u0026rsquo;t say, lol). I\u0026rsquo;ll later post my terraform script used to deploy it on AWS with more details. Now for the Grafana part:\nGrafana  Here\u0026rsquo;s how I configured a Bar gauge graph to see the PUT Histogram:\nWe can see that the majority of the operations take less than 0.5 seconds to finish, so if we start seeing an increase in the other histogram buckets, it means that we probably have a problem.\nI also created a panel to watch for errors and one to see all the operations combined:\nConclusion  That kind of monitoring can give a good observability to a service like Ceph S3 Gateway, but this could easily be applied to any other service with some changes. Before that, we used UpTimeRobot to achieve a similar goal, but from my perspective you can have much more control and flexibility with a container like that performing connectivity tests, and in this case we go much deeper by mimicking user operations on our tests.\n",
    "ref": "/blog/python-s3-monitoring/"
  },{
    "title": "Using Rook to leverage Ceph storage on a Kubernetes cluster",
    "date": "",
    "description": "We used Rook to leverage Ceph storage on some Kubernetes clusters with unused storage devices to provide a great option for persistent volumes.",
    "body": "I recently got 10 bare-metal servers to play with, they used to be part of our first Ceph cluster and got replaced with more powerful hardware. So i built two k8s clusters and decided to give Rook a try.\n As the cluster grew bigger, we purchased not only more servers but with different configuration. But those Dell R530 servers still have pretty decent power to run many internal demands we have, so i built a Ceph cluster with four of them using CentOS 8, Ceph Octopus and deployed everything in containers. But i want to talk about what i did with the remaining six servers, that became two kubernetes clusters - one with Flatcar and the other with Centos, to replicate some production scenarios we have. Since the servers have 8 2TB HDD each and we use just one for the OS, that would be a perfect scenario to experiment with Rook. I tinkered with it previously with some labs but now i could really test its usability.\nGetting Rook to run is pretty straightforward, it might not even be necessary to have great knowledge about ceph for the initial setup. However it would be mandatory to know about Ceph administration for a production environment. There are many administrative tasks that gets taken care of by Rook, but after working with Ceph for the past 3 years i can assure you that there will be many situations where the sysadmin will have to step up.\nI used the quickstart guide as a reference, it can be found on the Rook GitHub page: https://rook.github.io/docs/rook/v1.6/ceph-quickstart.html.\nThe only parameter i changed from the defaults was the ROOK_ENABLE_DISCOVERY_DAEMON, since i wanted it to automatically detect and provision the OSDs on every empty device. It\u0026rsquo;s important to erase the disks prior to the deployment, because it will skip any device with partitions. For that you can use wipefs or even dd. Be aware that if the device has LVM partitions and you want to avoid a reboot you might want to delete the LVs and VGs first. If you wipe the device the kernel might not release the mapped lvm volumes and you will not be able to delete after that. Needless to say that i made that mistake a few times before learning the lesson.\nCommands i used:\ngit clone --single-branch --branch master https://github.com/rook/rook.git\rgit checkout release-1.6\rcd rook/cluster/examples/kubernetes/ceph\rsed -i s/\u0026quot;ROOK_ENABLE_DISCOVERY_DAEMON: \\\u0026quot;false\\\u0026quot;\u0026quot;/\u0026quot;ROOK_ENABLE_DISCOVERY_DAEMON: \\\u0026quot;true\\\u0026quot;\u0026quot;/ operator.yaml\rkubectl create -f crds.yaml -f common.yaml -f operator.yaml\rkubectl create -f cluster.yaml\rChecking if the pods were created:\n$ kubectl -n rook-ceph get pod\rNAME READY STATUS RESTARTS AGE\rcsi-cephfsplugin-7tkgk 3/3 Running 25 95d\rcsi-cephfsplugin-cv2kq 3/3 Running 6 95d\rcsi-cephfsplugin-provisioner-bc5cff84-8h5b4 6/6 Running 19 95d\rcsi-cephfsplugin-provisioner-bc5cff84-cb477 6/6 Running 6 95d\rcsi-cephfsplugin-qvqjc 3/3 Running 3 95d\rcsi-rbdplugin-2rqwk 3/3 Running 25 95d\rcsi-rbdplugin-g5tkj 3/3 Running 3 95d\rcsi-rbdplugin-provisioner-97957587f-gvn6r 6/6 Running 0 22d\rcsi-rbdplugin-provisioner-97957587f-lflc7 6/6 Running 8 95d\rcsi-rbdplugin-vcsdj 3/3 Running 6 95d\rrook-ceph-crashcollector-node1-7fcbcd7dc6-6fbq6 1/1 Running 2 73d\rrook-ceph-crashcollector-node2-7f699c88c-78gvw 1/1 Running 0 22d\rrook-ceph-crashcollector-node3-77777995d8-dtsrh 1/1 Running 1 95d\rrook-ceph-mgr-a-8486cbdf64-dsdn2 1/1 Running 0 24d\rrook-ceph-mon-a-7758d4d54c-crq4s 1/1 Running 2 95d\rrook-ceph-mon-d-77db79f9b9-fcd5r 1/1 Running 0 41d\rrook-ceph-mon-e-7d44dcff6b-zjpfl 1/1 Running 0 22d\rrook-ceph-operator-66f7668857-sr8bw 1/1 Running 2 95d\rrook-ceph-osd-0-57b7d8b47b-k5ps8 1/1 Running 2 95d\rrook-ceph-osd-1-548c7bc54d-zsp8r 1/1 Running 1 95d\rrook-ceph-osd-10-7875d885d8-h2wdt 1/1 Running 0 22d\rrook-ceph-osd-11-5585fb7856-gxjfx 1/1 Running 2 95d\rrook-ceph-osd-12-fd5bcb8f8-z9v4c 1/1 Running 1 95d\rrook-ceph-osd-13-784c797d46-qmxj4 1/1 Running 0 22d\rrook-ceph-osd-14-6b689bfb4-sfffq 1/1 Running 2 95d\rrook-ceph-osd-15-64984bf7db-h99q2 1/1 Running 1 95d\rrook-ceph-osd-16-6f66f68868-ft8d8 1/1 Running 0 22d\rrook-ceph-osd-17-66fcdfc8c8-rg76t 1/1 Running 2 95d\rrook-ceph-osd-18-85f9d5567b-6jjph 1/1 Running 1 95d\rrook-ceph-osd-19-969b58fb-ztzxx 1/1 Running 0 22d\rrook-ceph-osd-2-ffc69957b-846mj 1/1 Running 2 95d\rrook-ceph-osd-20-5c86c5d556-26kkp 1/1 Running 0 22d\rrook-ceph-osd-3-79cb4b8f78-wxczm 1/1 Running 1 95d\rrook-ceph-osd-4-9587d6994-z94l2 1/1 Running 0 22d\rrook-ceph-osd-5-74cb556886-sqp6g 1/1 Running 2 95d\rrook-ceph-osd-6-5fb96bcb-2lgqr 1/1 Running 1 95d\rrook-ceph-osd-7-85f8659c9d-7z5tn 1/1 Running 0 22d\rrook-ceph-osd-8-6765676bfb-kjxr5 1/1 Running 2 95d\rrook-ceph-osd-9-67b4f98cdb-h9bd8 1/1 Running 1 95d\rrook-ceph-osd-prepare-node1-6x7dr 0/1 Completed 0 133m\rrook-ceph-osd-prepare-node2-z6jl8 0/1 Completed 0 133m\rrook-ceph-osd-prepare-node3-rvhth 0/1 Completed 0 133m\rrook-ceph-tools-6f58686b5d-znnsg 1/1 Running 0 24d\rrook-discover-2684s 1/1 Running 7 95d\rrook-discover-2nlkj 1/1 Running 1 95d\rrook-discover-4rhxm 1/1 Running 2 95d\rThe output is long but i wanted to show that Rook actually created a OSD por for every device in each node, and i configured kubernetes to allow that on the master node either.\nWe can see that we now have a bunch of new resource definitions, enabling us to interact with Ceph in a declarative way, instead of recurring to the CLI to manage users, filesystems, pools and many other resources:\n$ kubectl get crd | grep 'rook\\|objectbucket'\rcephblockpools.ceph.rook.io 2021-04-01T19:30:54Z\rcephclients.ceph.rook.io 2021-04-01T19:30:53Z\rcephclusters.ceph.rook.io 2021-04-01T19:30:53Z\rcephfilesystems.ceph.rook.io 2021-04-01T19:30:53Z\rcephnfses.ceph.rook.io 2021-04-01T19:30:53Z\rcephobjectrealms.ceph.rook.io 2021-04-01T19:30:54Z\rcephobjectstores.ceph.rook.io 2021-04-01T19:30:53Z\rcephobjectstoreusers.ceph.rook.io 2021-04-01T19:30:54Z\rcephobjectzonegroups.ceph.rook.io 2021-04-01T19:30:54Z\rcephobjectzones.ceph.rook.io 2021-04-01T19:30:54Z\rcephrbdmirrors.ceph.rook.io 2021-04-01T19:30:53Z\robjectbucketclaims.objectbucket.io 2021-04-01T19:30:54Z\robjectbuckets.objectbucket.io 2021-04-01T19:30:54Z\rvolumes.rook.io 2021-04-01T19:30:54Z\rFrom that point we can access the Ceph CLI by interacting with the rook-ceph-tools pod:\n$ kubectl exec -it -n rook-ceph rook-ceph-tools-6f58686b5d-znnsg -- ceph status\rcluster:\rid: 72e9c4a9-4315-45e0-ad43-cd79d22fb2be\rhealth: HEALTH_OK\rservices:\rmon: 3 daemons, quorum a,d,e (age 7d)\rmgr: a(active, since 3w)\rosd: 21 osds: 21 up (since 3w), 21 in (since 3w)\rrgw: 1 daemon active (my.store.a)\rtask status:\rdata:\rpools: 30 pools, 401 pgs\robjects: 2.12k objects, 2.4 GiB\rusage: 29 GiB used, 38 TiB / 38 TiB avail\rpgs: 401 active+clean\rio:\rclient: 7.0 KiB/s rd, 72 KiB/s wr, 6 op/s rd, 16 op/s wr\rNow we have two new StorageClasses to work with:\n$ kubectl get storageclasses\rNAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE\rrook-ceph-block rook-ceph.rbd.csi.ceph.com Delete Immediate false 64d\rrook-ceph-bucket rook-ceph.ceph.rook.io/bucket Delete Immediate false 84d\rAnd then we provisioned some volumes to a few applications running on the cluster:\n$ kubectl get pv -A\rNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE\rpvc-1c9151ae-3e9e-49ae-adbf-f365de4eb718 8Gi RWO Delete Bound cluster-spd/data-etcd-1 rook-ceph-block 18d\rpvc-39c5a513-1c50-48a9-9e4d-354ff84ea8b4 8Gi RWO Delete Bound cluster-spb/data-etcd-0 rook-ceph-block 24d\rpvc-53f2816a-e4c2-434d-8600-20206d3f6f75 8Gi RWO Delete Bound cluster-spc/data-etcd-0 rook-ceph-block 18d\rpvc-553c1a1b-0131-44a9-8a67-293dea40c161 5Gi RWO Delete Bound gitlab/redis-data-gitlab-redis-master-0 rook-ceph-block 56d\rpvc-6c38a9b7-8e23-4478-a431-34fd4f9ec158 8Gi RWO Delete Bound cluster-spd/data-etcd-2 rook-ceph-block 18d\rpvc-76bfab8e-6ae6-4b11-8223-ea9e97bbccf4 8Gi RWO Delete Bound cluster-spc/data-etcd-2 rook-ceph-block 18d\rpvc-836c1ff1-c92d-467c-b45b-b970824e2b04 8Gi RWO Delete Bound cluster-spc/data-etcd-1 rook-ceph-block 18d\rpvc-87e7bd8e-250e-4bd4-8268-3fb49597d0ab 10Gi RWO Delete Bound gitlab/gitlab-minio rook-ceph-block 39d\rpvc-bac3dc77-73ae-4800-821a-04faa15e627d 8Gi RWO Delete Bound cluster-spb/data-etcd-1 rook-ceph-block 24d\rpvc-ca087b82-038b-418a-a187-8e28617fbfae 8Gi RWO Delete Bound cluster-spd/data-etcd-0 rook-ceph-block 18d\rpvc-d255ca76-b62f-413b-82b8-184ff3cb0026 8Gi RWO Delete Bound cluster-spb/data-etcd-2 rook-ceph-block 24d\rpvc-d331d766-72d7-46c4-9cba-c0e8fea7a2b8 50Gi RWO Delete Bound gitlab/repo-data-gitlab-gitaly-0 rook-ceph-block 56d\rpvc-e76d375b-03b5-4bc0-8725-bc406cb7ee94 8Gi RWO Delete Bound gitlab/data-gitlab-postgresql-0 rook-ceph-block 56d\rThis is how the pvc manifest looks like:\n$ kubectl get pvc -n gitlab gitlab-minio -o yaml\rapiVersion: v1\rkind: PersistentVolumeClaim\rmetadata:\rname: gitlab-minio\rnamespace: gitlab\rlabels:\rapp: minio\rspec:\rstorageClassName: rook-ceph-block\raccessModes:\r- ReadWriteOnce\rresources:\rrequests:\rstorage: 10Gi\rWe can also provision Buckets using S3 via Rook. We used this to provide a backend for a Hashicorp Vault we have on the same environment:\nFirst we have to create the StorageClass:\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: rook-ceph-bucket\rprovisioner: rook-ceph.ceph.rook.io/bucket\rreclaimPolicy: Delete\rparameters:\robjectStoreName: my-store\robjectStoreNamespace: rook-ceph\rregion: us-east-1\rThen we create a ObjectBucketClaim, which is a new resource type defined by the Rook CRD:\napiVersion: objectbucket.io/v1alpha1\rkind: ObjectBucketClaim\rmetadata:\rname: ceph-bucket\rnamespace: vault\rspec:\rgenerateBucketName: ceph-bucket\rstorageClassName: rook-ceph-bucket\r",
    "ref": "/blog/rook/"
  },{
    "title": "Web scrapper using Python",
    "date": "",
    "description": "Scrapping FIIs using Python",
    "body": "So I needed to learn how to scrape a web page using Python.\n My teammate suggested that I could learn a few tricks in python by scraping FIIs, which is the brazilian equivalent to american REITs. I did some research and found some good examples using a module called pandas (https://realpython.com/python-csv/) , a tool designed for data analisys and manipulation and very popular among people working with data science. My goal was much less ambitious but i thought i might as well use what everyone else is using, since it would provide a good learning opportunity.\nI based my code on Renata Magner\u0026rsquo;s (https://github.com/RenataMagner/web_scraping_fii/blob/master/web_scraping-fiiv2.ipynb). I still have to implement error handling and many other improvements on the code, that\u0026rsquo;s just the first version. Without further ado, that\u0026rsquo;s what i came up with:\nfrom bs4 import BeautifulSoup import requests import pandas as pd url = 'https://www.fundsexplorer.com.br/ranking' html = requests.get(url).content soup = BeautifulSoup(html, 'html.parser') # stores scraped data on a list results = soup.find_all(\u0026quot;td\u0026quot;) # counts how many entries we got. each entry has 26 'td' lines each. fii_count=int(len(results)/26) # initialize the variables fii_info={} fii_list=[] ''' index - field [0] - COD FUNDO [1] - SETOR [2] - PRECO_ATUAL [3] - LIQUIDEZ DIARIA [4] - DIVIDENDO [5] - dividend_yield [6] - dy_3m [7] - dy_6m [8] - dy_12m [9]- dy_3m_media [10] - dy_6m_media [11] - dy_12m_media [12] - dy_ano [13] - variacao_preco [14] - rentabilidade_periodo [15] - rentabilidade_acumulada [16] - patrimonio_liq [17] - vpa [18] - p_vpa ''' # runs through collected data and extracts the desided info for fii in range(fii_count): fii_info['codigo_fundo']=results[fii*26].get_text() fii_info['preco_atual']=results[fii*26+2].get_text() fii_info['setor']=results[fii*26+1].get_text() fii_info['p/vpa']= results[fii*26+18].get_text() fii_info['dividend_yield']=results[fii*26+5].get_text() fii_info['dividendo']=results[fii*26+4].get_text() fii_info['dy_12m_media']=results[fii*26+11].get_text() fii_list.append(fii_info) fii_info={} # generates the CSV output fii_table = pd.DataFrame(data=fii_list) fii_table.to_csv('fii_table.csv') References:\nhttps://github.com/RenataMagner/web_scraping_fii/blob/master/web_scraping-fiiv2.ipynb\nhttps://realpython.com/python-csv/\nhttps://www.fundsexplorer.com.br/ranking\n",
    "ref": "/blog/python-web-scrapper-fii/"
  },{
    "title": "Comparsion of Sorting Algorithms using Python Decorators",
    "date": "",
    "description": "Measuring runtime of sorting algorithms using Python decorators",
    "body": "So I needed to learn how to use decorators in Python.\n I tried to come up with something within my current knowledge and since I was studying sorting algorithms, I decided to use decorators to measure how long each algorithm would take to complete and compare them. I know that it is kind of obvious which one is better in the proposed scenario for anyone with a Bachelor in Computer Science or any experienced developer, but the purpose here was to learn and develop my python skills. Decorators can be very useful when you need to add functionalities to an existing function/method/class, without having to change its code. Let\u0026rsquo;s say you need to add an additional check, time the function or log but you are using a module instead of writing your own function, or you want to reuse the code more efficiently, then using decorators might be the way to go.\nThere are many other ways to use decorators that i still have to explore, like chained decorators for instance.\nI found a good post at https://kleiber.me/blog/2017/08/10/tutorial-decorator-primer/ , where the author uses the same example I used (comparing sorting algorithms), but he used the pygorithm.sorting module, while I used my own functions.\nThere are many different ways to approach this, and we can always improve our older codes as we get better at a programming language. With that in mind, here\u0026rsquo;s the code i came up with, using Python 3:\nimport random import functools import time # define the function to measure time def timeIt(func): # wraps the function using functools @functools.wraps(func) def newfunc(*args, **kwargs): # check to enter the function only once if not hasattr(newfunc, '_entered'): newfunc._entered = True # starts the measurement startTime = time.time() func(*args, **kwargs) # finishes the measurement elapsedTime = time.time() - startTime # prints the result print('function [{}] finished in {} ms'.format( func.__name__, int(elapsedTime * 1000))) del newfunc._entered return newfunc # the @ inserts decorator @timeIt def mergeSort(L): if len(L) \u0026gt; 1: mid = len(L) // 2 left = L[:mid] right = L[mid:] mergeSort(left) mergeSort(right) i = j = k = 0 while i \u0026lt; len(left) and j \u0026lt; len(right): if left[i] \u0026lt; right[j]: L[k] = left[i] i += 1 else: L[k] = right[j] j += 1 k += 1 while i \u0026lt; len(left): L[k] = left[i] i += 1 k += 1 while j \u0026lt; len(right): L[k] = right[j] j += 1 k += 1 @timeIt def selectionSort(L): for i in range(0, len(L)): min_i = i for right in range(i + 1, len(L)): if L[right] \u0026lt; L[min_i]: min_i = right L[i], L[min_i] = L[min_i], L[i] @timeIt def bubbleSort(L): elem = len(L) - 1 issorted = False while not issorted: issorted = True for i in range(elem): if L[i] \u0026gt; L[i + 1]: L[i], L[i + 1] = L[i + 1], L[i] issorted = False randomList = random.sample(range(5000), 5000) mergeSort(randomList.copy()) selectionSort(randomList.copy()) bubbleSort(randomList.copy()) And the results:\nfunction [mergeSort] finished in 2 ms function [selectionSort] finished in 930 ms function [bubbleSort] finished in 2828 ms ecco!\nReferences:\nhttps://docs.python.org/3/library/timeit.html\nhttps://stackoverflow.com/questions/5478351/python-time-measure-function\n",
    "ref": "/blog/python-sort-comparsion/"
  },{
    "title": "About",
    "date": "",
    "description": "More about me!",
    "body": "Soon\u0026hellip; ¯\\_(ツ)_/¯\n",
    "ref": "/about/"
  },{
    "title": "Ceph features and its relation to the client kernel version",
    "date": "",
    "description": "How the kernel version used on rbd/cephfs clients reflects on ceph features from the cluster perspective",
    "body": "While working on an upgrade on one of our ceph cluster from Luminous to Nautilus, i needed to come up with a way to detect any client with older versions, and check if we could break anything after the upgrade.\n I started checking the documentation as always, but all i found was a command called ceph features, which gives a summarized output:\n[root@mon-1 ~]# ceph features { \u0026quot;mon\u0026quot;: { \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x3ffddff8eeacfffb\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;luminous\u0026quot;, \u0026quot;num\u0026quot;: 3 } }, \u0026quot;mds\u0026quot;: { \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x3ffddff8eeacfffb\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;luminous\u0026quot;, \u0026quot;num\u0026quot;: 3 } }, \u0026quot;osd\u0026quot;: { \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x3ffddff8eeacfffb\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;luminous\u0026quot;, \u0026quot;num\u0026quot;: 192 } }, \u0026quot;client\u0026quot;: { \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x27018fb86aa42ada\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;jewel\u0026quot;, \u0026quot;num\u0026quot;: 422 }, \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x2f018fb86aa42ada\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;luminous\u0026quot;, \u0026quot;num\u0026quot;: 95 }, \u0026quot;group\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x3ffddff8eeacfffb\u0026quot;, \u0026quot;release\u0026quot;: \u0026quot;luminous\u0026quot;, \u0026quot;num\u0026quot;: 18 } } } So that got me worried, since I was now looking at 422 clients apparently using jewel and we were already using Nautilus for a while. Since this specific cluster was \u0026ldquo;born\u0026rdquo; in jewel, I thought that it might be the case that many clients didn\u0026rsquo;t get any upgrades since.\nAnother way to see who\u0026rsquo;s connected to the cluster is the command ceph tell mds.\\hostname` client ls`, but it shows only cephfs clients, which solves only a part of the problem. At least we can see the client\u0026rsquo;s IP address (adresses and names omitted with [\u0026hellip;]):\n\u0026quot;inst\u0026quot;: \u0026quot;client.1400410 v1:[...]]:0/1769731892\u0026quot;, \u0026quot;completed_requests\u0026quot;: [], \u0026quot;prealloc_inos\u0026quot;: [], \u0026quot;used_inos\u0026quot;: [], \u0026quot;client_metadata\u0026quot;: { \u0026quot;features\u0026quot;: \u0026quot;0x00000000000001ff\u0026quot;, \u0026quot;entity_id\u0026quot;: [...], \u0026quot;hostname\u0026quot;: [...], \u0026quot;kernel_version\u0026quot;: \u0026quot;4.19.[...]\u0026quot;, \u0026quot;root\u0026quot;: [...] } Short note: there\u0026rsquo;s no documentation explaining those codes and its respective features, so I\u0026rsquo;m going to look at the source code and try to come up with a table. I\u0026rsquo;ll post it as soon as I manage to.\nSearching further i came across the command ceph daemon mon.hostname sessions:\n[root@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep jewel \u0026quot;MonSession(client.354037 v1:[...]]:0/3173650400 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.394132 v1:[...]:0/295867623 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.785407 v1:[...]:0/507474248 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.367883 v1:[...]:0/3274665177 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.812187 v1:[...]:0/733157529 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.1390811 v1:[...]:0/1470215921 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.1400452 v1:[...]:0/3851374868 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, Now i started to see some light at the end, since that command gave me the ip address and the version for each client connected. I knew that we didn\u0026rsquo;t have any clients using ceph-fuse, so it was safe to assume that everyone was using the kernel module to mount either RBD or cephfs. That cleared some confusion I had at the beginning between the ceph-common package version and the kernel version. It is not required to have that package installed in order to use the kernel module, so i then pointed my efforts at checking the clients' kernel versions.\nThe documentation also recommended some debugging at mon:\n The ceph features command that reports the total number of clients and daemons and their features and releases. If the debugging level for Monitors is set to 10 (debug mon = 10), addresses and features of connecting and disconnecting clients are logged to log file on a local file system.\n So i tried that and here\u0026rsquo;s what we get with debug mon = 10:\n2020-11-10 08:55:59.084 7f31c9015700 0 --1- [v2:[...]:3300/0,v1:[...]:6789/0] \u0026gt;\u0026gt; conn(0x561a94aed180 0x561a94ba5000 :6789 s=ACCE PTING pgs=0 cs=0 l=0).handle_client_banner accept peer addr is really - (socket is v1:[...]:40764/0) 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader) e1 ms_handle_accept con 0x561a94aed180 no session 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader) e1 _ms_dispatch new session 0x561a94adcd80 MonSession(unknown.0 - is open , featu res 0x27018fb86aa42ada (jewel)) features 0x27018fb86aa42ada 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 preprocess_query auth(proto 0 34 bytes epoch 0) from unknown.0 - 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 prep_auth() blob_size=34 2020-11-10 08:55:59.085 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 _assign_global_id 34110 (max 44096) 2020-11-10 08:55:59.085 7f31c500d700 2 mon.ceph-1@0(leader) e1 send_reply 0x561a958ba780 0x561a95246fc0 auth_reply(proto 2 0 (0) Success) v1 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 preprocess_query auth(proto 2 32 bytes epoch 0) from unknown.0 - 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader).auth v3676 prep_auth() blob_size=32 2020-11-10 08:55:59.086 7f31c500d700 10 mon.ceph-1@0(leader) e1 ms_handle_authentication session 0x561a94adcd80 con 0x561a94aed180 addr - MonSession(unknown.0 - is open , features 0x27018fb86aa42ada (jewel)) Without debug mon = 10, only the first line is logged.\nThat is very useful to detect the clients' features when they are connecting, but not so much when you can\u0026rsquo;t have the luxury to reset everyone\u0026rsquo;s connection. Besides, leaving that level of debugging enabled all the time would flag our node as an abuser at the log server in a matter of hours.\nWhat i knew already about our clients was that there was some elastic nodes using RBD, probably with CentOS 7 and consequently using kernel 3.x; Some other stuff also using RBD on either CentOS 7 or CentOS 8; and a kubernetes cluster using CoreOS and Flatcar with kernel 4.x. So after some checking, I compared the output from a client using kernel 4.19.x and another with 5.4.x:\n4.19.x:\n[root@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep [...] \u0026quot;MonSession(client.95408384 [...]:0/923650579 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.95446387 [...]:0/421761626 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(client.95427342 [...]:0/1106672011 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/2823489235 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/4141488860 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/662892667 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/940509445 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/1147570690 is open allow r, features 0x27018fb86aa42ada (jewel))\u0026quot;, 5.4.x:\nroot@mon-1 ~]# ceph daemon mon.`hostname` sessions | grep [...] \u0026quot;MonSession(unknown.0 [...]:0/2258512824 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/4186418195 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/232634731 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(client.95587947 [...]:0/3963151684 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/1164059037 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/1228306108 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/2811111809 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/4254695077 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(client.95587926 [...]:0/479729343 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/340770437 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, \u0026quot;MonSession(unknown.0 [...]:0/425381347 is open allow r, features 0x2f018fb86aa42ada (luminous))\u0026quot;, After that I could come with some conclusion. We\u0026rsquo;ll probably keep seeing jewel features for a long time, since CentOS/RHEL 8 and most of the main distros uses kernel 4.x on their latest LTS versions. I expected that at kernel 3.x i\u0026rsquo;d get Jewel features and from 4.x on we would get Luminous, but apparently the latest features are only available on kernels 5.x and beyond. I\u0026rsquo;m not sure if there is any feature that would be marked as nautilus or any newer ceph version, so I still need to dig deeper on that subject.\nReferences:\nhttps://ceph.io/community/new-luminous-upgrade-complete/\nhttps://docs.ceph.com/en/latest/rados/operations/upmap/#enabling\nhttps://docs.ceph.com/en/latest/man/8/ceph/#osd\nhttps://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3.0/html/release_notes/major_updates\n",
    "ref": "/blog/ceph-features/"
  },{
    "title": "Obtaining a list of Ceph features from the hexadecimal value",
    "date": "",
    "description": "Simple python script that gets a hexadecimal value from the command 'ceph features' and outputs a list of the actual features.",
    "body": "After some digging i found the list of possible features, its respective kernel version requirement and/or when it became available.\n This can be found in src/include/ceph_features.h, and I got it by cloning the ceph repo at git://github.com/ceph/ceph .\nThat information might be useful when you are trying to determine if your clients might need a kernel upgrade and what kind of RBD or cephfs features you can enable on your server side without breaking compatibility. It\u0026rsquo;s always ideal to have every new feature availabe whenever possible, but that is not always the case when you have a medium to large deployment, multiple clients with different workloads and scenarios - in other words, a Real World situation.\nimport sys feature_list = [ \u0026quot;( 0, 1, UID)\u0026quot;, \u0026quot;( 1, 1, NOSRCADDR) // 2.6.35 req\u0026quot;, \u0026quot;( 2, 3, SERVER_NAUTILUS)\u0026quot;, \u0026quot;( 3, 1, FLOCK) // 2.6.36\u0026quot;, \u0026quot;( 4, 1, SUBSCRIBE2) // 4.6 req\u0026quot;, \u0026quot;( 5, 1, MONNAMES)\u0026quot;, \u0026quot;( 6, 1, RECONNECT_SEQ) // 3.10 req\u0026quot;, \u0026quot;( 7, 1, DIRLAYOUTHASH) // 2.6.38\u0026quot;, \u0026quot;( 8, 1, OBJECTLOCATOR)\u0026quot;, \u0026quot;( 9, 1, PGID64) // 3.9 req\u0026quot;, \u0026quot;(10, 1, INCSUBOSDMAP)\u0026quot;, \u0026quot;(11, 1, PGPOOL3) // 3.9 req\u0026quot;, \u0026quot;(12, 1, OSDREPLYMUX)\u0026quot;, \u0026quot;(13, 1, OSDENC) // 3.9 req\u0026quot;, \u0026quot;(14, 2, SERVER_KRAKEN)\u0026quot;, \u0026quot;(15, 1, MONENC)\u0026quot;, \u0026quot;(16, 3, SERVER_OCTOPUS) | (16, 3, OSD_REPOP_MLCOD)\u0026quot;, \u0026quot;(17, 3, OS_PERF_STAT_NS)\u0026quot;, \u0026quot;(18, 1, CRUSH_TUNABLES) // 3.6\u0026quot;, \u0026quot;(19, 2, OSD_PGLOG_HARDLIMIT)\u0026quot;, \u0026quot;(20, 3, SERVER_PACIFIC)\u0026quot;, \u0026quot;(21, 2, SERVER_LUMINOUS) // 4.13 | (21, 2, RESEND_ON_SPLIT) | (21, 2, RADOS_BACKOFF) | (21, 2, OSDMAP_PG_UPMAP) | (21, 2, CRUSH_CHOOSE_ARGS)\u0026quot;, \u0026quot;(22, 2, OSD_FIXED_COLLECTION_LIST)\u0026quot;, \u0026quot;(23, 1, MSG_AUTH) // 3.19 req (unless nocephx_require_signatures)\u0026quot;, \u0026quot;(24, 2, RECOVERY_RESERVATION_2)\u0026quot;, \u0026quot;(25, 1, CRUSH_TUNABLES2) // 3.9\u0026quot;, \u0026quot;(26, 1, CREATEPOOLID)\u0026quot;, \u0026quot;(27, 1, REPLY_CREATE_INODE) // 3.9\u0026quot;, \u0026quot;(28, 2, SERVER_MIMIC)\u0026quot;, \u0026quot;(29, 1, MDSENC) // 4.7\u0026quot;, \u0026quot;(30, 1, OSDHASHPSPOOL) // 3.9\u0026quot;, \u0026quot;DEPRECATED(31, 1, MON_SINGLE_PAXOS, NAUTILUS)\u0026quot;, \u0026quot;(32, 3, STRETCH_MODE)\u0026quot;, \u0026quot;RETIRED(33, 1, MON_SCRUB, JEWEL, LUMINOUS)\u0026quot;, \u0026quot;RETIRED(34, 1, OSD_PACKED_RECOVERY, JEWEL, LUMINOUS)\u0026quot;, \u0026quot;(35, 1, OSD_CACHEPOOL) // 3.14\u0026quot;, \u0026quot;(36, 1, CRUSH_V2) // 3.14\u0026quot;, \u0026quot;(37, 1, EXPORT_PEER) // 3.14\u0026quot;, \u0026quot;RETIRED(38, 1, OSD_ERASURE_CODES, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;(39, 1, OSDMAP_ENC) // 3.15\u0026quot;, \u0026quot;(40, 1, MDS_INLINE_DATA) // 3.19\u0026quot;, \u0026quot;(41, 1, CRUSH_TUNABLES3) // 3.15 | (41, 1, OSD_PRIMARY_AFFINITY)\u0026quot;, \u0026quot;(42, 1, MSGR_KEEPALIVE2) // 4.3 (for consistency)\u0026quot;, \u0026quot;(43, 1, OSD_POOLRESEND) // 4.13\u0026quot;, \u0026quot;RETIRED(44, 1, ERASURE_CODE_PLUGINS_V2, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(45, 1, OSD_SET_ALLOC_HINT, JEWEL, LUMINOUS)\u0026quot;, \u0026quot;(46, 1, OSD_FADVISE_FLAGS)\u0026quot;, \u0026quot;(47, 1, MDS_QUOTA) // 4.17\u0026quot;, \u0026quot;(48, 1, CRUSH_V4) // 4.1\u0026quot;, \u0026quot;RETIRED(49, 1, OSD_MIN_SIZE_RECOVERY, JEWEL, LUMINOUS)\u0026quot;, \u0026quot;RETIRED(50, 1, MON_METADATA, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(51, 1, OSD_BITWISE_HOBJ_SORT, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(52, 1, OSD_PROXY_WRITE_FEATURES, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(53, 1, ERASURE_CODE_PLUGINS_V3, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(54, 1, OSD_HITSET_GMT, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;RETIRED(55, 1, HAMMER_0_94_4, MIMIC, OCTOPUS)\u0026quot;, \u0026quot;(56, 1, NEW_OSDOP_ENCODING) // 4.13 (for pg_pool_t \u0026gt;= v25)\u0026quot;, \u0026quot;(57, 1, MON_STATEFUL_SUB) // 4.13 | (57, 1, SERVER_JEWEL)\u0026quot;, \u0026quot;(58, 1, CRUSH_TUNABLES5) // 4.5 | (58, 1, NEW_OSDOPREPLY_ENCODING) | (58, 1, FS_FILE_LAYOUT_V2)\u0026quot;, \u0026quot;(59, 1, FS_BTIME) | (59, 1, FS_CHANGE_ATTR) | (59, 1, MSG_ADDR2)\u0026quot;, \u0026quot;(60, 1, OSD_RECOVERY_DELETES) // *do not share this bit*\u0026quot;, \u0026quot;(61, 1, CEPHX_V2) // 4.19, *do not share this bit*\u0026quot;, \u0026quot;(62, 1, RESERVED) // do not use; used as a sentinel\u0026quot;, \u0026quot;DEPRECATED(63, 1, RESERVED_BROKEN, LUMINOUS) // client-facing\u0026quot; ] def settolist(featureset): '''Converts a featureset hex to a feature list. Parameters: int:featureset Returns: list:featureset_list ''' featureset_hex = featureset featureset_list = [] featureset_bin = bin(int(featureset_hex, 16))[2:].zfill(64) count = 0 for bit in (featureset_bin): if bit == '1': featureset_list.append(feature_list[count]) count += 1 return featureset_list output = settolist(str(sys.argv[1])) print(\u0026quot;\\n\u0026quot;.join(output)) A test run using an entry from my test cluster:\nMonSession(client.95280473 [IP]:0/110518927 is open allow profile rbd, features 0x27018fb86aa42ada (jewel))\u0026quot;\npython features.py 0x27018fb86aa42ada ( 2, 3, SERVER_NAUTILUS) ( 5, 1, MONNAMES) ( 6, 1, RECONNECT_SEQ) // 3.10 req ( 7, 1, DIRLAYOUTHASH) // 2.6.38 (15, 1, MONENC) (16, 3, SERVER_OCTOPUS) | (16, 3, OSD_REPOP_MLCOD) (20, 3, SERVER_PACIFIC) (21, 2, SERVER_LUMINOUS) // 4.13 | (21, 2, RESEND_ON_SPLIT) | (21, 2, RADOS_BACKOFF) | (21, 2, OSDMAP_PG_UPMAP) | (21, 2, CRUSH_CHOOSE_ARGS) (22, 2, OSD_FIXED_COLLECTION_LIST) (23, 1, MSG_AUTH) // 3.19 req (unless nocephx_require_signatures) (24, 2, RECOVERY_RESERVATION_2) (26, 1, CREATEPOOLID) (27, 1, REPLY_CREATE_INODE) // 3.9 (28, 2, SERVER_MIMIC) RETIRED(33, 1, MON_SCRUB, JEWEL, LUMINOUS) RETIRED(34, 1, OSD_PACKED_RECOVERY, JEWEL, LUMINOUS) (36, 1, CRUSH_V2) // 3.14 RETIRED(38, 1, OSD_ERASURE_CODES, MIMIC, OCTOPUS) (40, 1, MDS_INLINE_DATA) // 3.19 (42, 1, MSGR_KEEPALIVE2) // 4.3 (for consistency) RETIRED(45, 1, OSD_SET_ALLOC_HINT, JEWEL, LUMINOUS) RETIRED(50, 1, MON_METADATA, MIMIC, OCTOPUS) RETIRED(52, 1, OSD_PROXY_WRITE_FEATURES, MIMIC, OCTOPUS) RETIRED(54, 1, OSD_HITSET_GMT, MIMIC, OCTOPUS) (56, 1, NEW_OSDOP_ENCODING) // 4.13 (for pg_pool_t \u0026gt;= v25) (57, 1, MON_STATEFUL_SUB) // 4.13 | (57, 1, SERVER_JEWEL) (59, 1, FS_BTIME) | (59, 1, FS_CHANGE_ATTR) | (59, 1, MSG_ADDR2) (60, 1, OSD_RECOVERY_DELETES) // *do not share this bit* (62, 1, RESERVED) // do not use; used as a sentinel ",
    "ref": "/blog/ceph-features-2/"
  },{
    "title": "Contact",
    "date": "",
    "description": "",
    "body": "",
    "ref": "/contact/"
  }]
